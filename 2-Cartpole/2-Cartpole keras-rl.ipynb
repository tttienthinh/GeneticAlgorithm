{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-19 22:59:50.001646: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-19 22:59:50.001683: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/venv/lib/python3.8/site-packages/rl/agents/dqn.py line 108\n",
    "-        if hasattr(model.output, '__len__') and len(model.output) > 1:\n",
    "+        if hasattr(model.output, '__len__'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "# model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tttienthinh/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "  126/10000: episode: 1, duration: 0.767s, episode steps: 126, steps per second: 164, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.994613, mae: 28.018042, mean_q: 56.470532\n",
      "  253/10000: episode: 2, duration: 0.879s, episode steps: 127, steps per second: 144, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 3.072160, mae: 28.036873, mean_q: 56.639156\n",
      "  392/10000: episode: 3, duration: 0.940s, episode steps: 139, steps per second: 148, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  loss: 2.938457, mae: 28.050371, mean_q: 56.579521\n",
      "  592/10000: episode: 4, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.406415, mae: 28.269766, mean_q: 57.072075\n",
      "  753/10000: episode: 5, duration: 1.041s, episode steps: 161, steps per second: 155, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 3.338283, mae: 28.471735, mean_q: 57.361584\n",
      "  953/10000: episode: 6, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.815024, mae: 28.896416, mean_q: 58.239849\n",
      " 1146/10000: episode: 7, duration: 1.279s, episode steps: 193, steps per second: 151, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 2.451623, mae: 29.162247, mean_q: 58.784153\n",
      " 1308/10000: episode: 8, duration: 1.041s, episode steps: 162, steps per second: 156, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 3.628565, mae: 29.525919, mean_q: 59.381428\n",
      " 1508/10000: episode: 9, duration: 1.278s, episode steps: 200, steps per second: 156, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.626222, mae: 29.438086, mean_q: 59.303707\n",
      " 1688/10000: episode: 10, duration: 1.143s, episode steps: 180, steps per second: 157, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.341088, mae: 29.582954, mean_q: 59.698116\n",
      " 1862/10000: episode: 11, duration: 1.099s, episode steps: 174, steps per second: 158, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.537316, mae: 29.663546, mean_q: 59.778603\n",
      " 1984/10000: episode: 12, duration: 0.776s, episode steps: 122, steps per second: 157, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 2.935231, mae: 30.280193, mean_q: 61.063019\n",
      " 2179/10000: episode: 13, duration: 1.256s, episode steps: 195, steps per second: 155, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3.048572, mae: 30.063402, mean_q: 60.644913\n",
      " 2321/10000: episode: 14, duration: 0.891s, episode steps: 142, steps per second: 159, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.713616, mae: 30.361328, mean_q: 61.336544\n",
      " 2479/10000: episode: 15, duration: 1.007s, episode steps: 158, steps per second: 157, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 2.087812, mae: 30.556555, mean_q: 61.705498\n",
      " 2649/10000: episode: 16, duration: 1.061s, episode steps: 170, steps per second: 160, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.648205, mae: 30.847019, mean_q: 62.307053\n",
      " 2807/10000: episode: 17, duration: 1.015s, episode steps: 158, steps per second: 156, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 4.375131, mae: 31.142782, mean_q: 62.670124\n",
      " 2989/10000: episode: 18, duration: 1.163s, episode steps: 182, steps per second: 157, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 3.543233, mae: 31.067087, mean_q: 62.576054\n",
      " 3189/10000: episode: 19, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 2.776052, mae: 31.302711, mean_q: 63.154873\n",
      " 3364/10000: episode: 20, duration: 1.129s, episode steps: 175, steps per second: 155, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 5.056893, mae: 31.452644, mean_q: 63.347248\n",
      " 3564/10000: episode: 21, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.103496, mae: 31.528814, mean_q: 63.543713\n",
      " 3723/10000: episode: 22, duration: 1.011s, episode steps: 159, steps per second: 157, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 4.337625, mae: 31.919634, mean_q: 64.303215\n",
      " 3907/10000: episode: 23, duration: 1.172s, episode steps: 184, steps per second: 157, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 4.108133, mae: 32.030396, mean_q: 64.549919\n",
      " 4069/10000: episode: 24, duration: 1.026s, episode steps: 162, steps per second: 158, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 5.085712, mae: 31.880720, mean_q: 64.257706\n",
      " 4269/10000: episode: 25, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.153987, mae: 32.247555, mean_q: 65.036476\n",
      " 4466/10000: episode: 26, duration: 1.310s, episode steps: 197, steps per second: 150, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 4.047420, mae: 32.287971, mean_q: 65.156410\n",
      " 4617/10000: episode: 27, duration: 0.955s, episode steps: 151, steps per second: 158, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 3.582076, mae: 32.465832, mean_q: 65.539383\n",
      " 4810/10000: episode: 28, duration: 1.190s, episode steps: 193, steps per second: 162, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 4.644670, mae: 32.559929, mean_q: 65.621780\n",
      " 5009/10000: episode: 29, duration: 1.276s, episode steps: 199, steps per second: 156, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 4.164204, mae: 32.703308, mean_q: 65.996506\n",
      " 5209/10000: episode: 30, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.915680, mae: 33.074062, mean_q: 66.803932\n",
      " 5378/10000: episode: 31, duration: 1.057s, episode steps: 169, steps per second: 160, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.334691, mae: 33.004890, mean_q: 66.638916\n",
      " 5560/10000: episode: 32, duration: 1.125s, episode steps: 182, steps per second: 162, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.858201, mae: 33.453121, mean_q: 67.499466\n",
      " 5726/10000: episode: 33, duration: 1.064s, episode steps: 166, steps per second: 156, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.533723, mae: 33.300789, mean_q: 67.284729\n",
      " 5902/10000: episode: 34, duration: 1.125s, episode steps: 176, steps per second: 157, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 5.050732, mae: 33.494221, mean_q: 67.548393\n",
      " 6102/10000: episode: 35, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.921612, mae: 33.782627, mean_q: 68.125420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6293/10000: episode: 36, duration: 1.249s, episode steps: 191, steps per second: 153, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 4.957035, mae: 33.986820, mean_q: 68.533287\n",
      " 6486/10000: episode: 37, duration: 1.307s, episode steps: 193, steps per second: 148, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.753073, mae: 34.289268, mean_q: 69.209969\n",
      " 6677/10000: episode: 38, duration: 1.252s, episode steps: 191, steps per second: 153, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 4.172369, mae: 34.320705, mean_q: 69.228783\n",
      " 6850/10000: episode: 39, duration: 1.115s, episode steps: 173, steps per second: 155, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4.242710, mae: 34.592770, mean_q: 69.747993\n",
      " 7030/10000: episode: 40, duration: 1.176s, episode steps: 180, steps per second: 153, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.772167, mae: 34.506927, mean_q: 69.734291\n",
      " 7225/10000: episode: 41, duration: 1.361s, episode steps: 195, steps per second: 143, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 4.401915, mae: 34.634377, mean_q: 69.719124\n",
      " 7425/10000: episode: 42, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.241787, mae: 35.202381, mean_q: 70.923996\n",
      " 7623/10000: episode: 43, duration: 1.327s, episode steps: 198, steps per second: 149, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 4.300764, mae: 35.105640, mean_q: 70.764145\n",
      " 7823/10000: episode: 44, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.266240, mae: 35.216820, mean_q: 71.029884\n",
      " 7972/10000: episode: 45, duration: 0.949s, episode steps: 149, steps per second: 157, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 4.956860, mae: 35.470970, mean_q: 71.568459\n",
      " 8132/10000: episode: 46, duration: 1.111s, episode steps: 160, steps per second: 144, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 6.809429, mae: 35.583569, mean_q: 71.555687\n",
      " 8289/10000: episode: 47, duration: 1.024s, episode steps: 157, steps per second: 153, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 3.624179, mae: 35.961449, mean_q: 72.603096\n",
      " 8469/10000: episode: 48, duration: 1.177s, episode steps: 180, steps per second: 153, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 6.134840, mae: 36.098366, mean_q: 72.887474\n",
      " 8657/10000: episode: 49, duration: 1.231s, episode steps: 188, steps per second: 153, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 5.798289, mae: 36.180519, mean_q: 72.939484\n",
      " 8826/10000: episode: 50, duration: 1.101s, episode steps: 169, steps per second: 154, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4.077638, mae: 36.660919, mean_q: 73.963142\n",
      " 9026/10000: episode: 51, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.533354, mae: 36.627064, mean_q: 73.875320\n",
      " 9196/10000: episode: 52, duration: 1.127s, episode steps: 170, steps per second: 151, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 5.897904, mae: 36.718346, mean_q: 73.933838\n",
      " 9387/10000: episode: 53, duration: 1.297s, episode steps: 191, steps per second: 147, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 6.090202, mae: 36.849659, mean_q: 74.366776\n",
      " 9559/10000: episode: 54, duration: 1.197s, episode steps: 172, steps per second: 144, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 5.210779, mae: 36.968075, mean_q: 74.457642\n",
      " 9753/10000: episode: 55, duration: 1.312s, episode steps: 194, steps per second: 148, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 4.308497, mae: 37.335743, mean_q: 75.333893\n",
      " 9945/10000: episode: 56, duration: 1.293s, episode steps: 192, steps per second: 148, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 5.880005, mae: 37.263023, mean_q: 75.159927\n",
      "done, took 65.176 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f502ae4b0a0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=10_000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 188.000, steps: 188\n",
      "Episode 2: reward: 181.000, steps: 181\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 172.000, steps: 172\n",
      "Episode 7: reward: 196.000, steps: 196\n",
      "Episode 8: reward: 168.000, steps: 168\n",
      "Episode 9: reward: 182.000, steps: 182\n",
      "Episode 10: reward: 178.000, steps: 178\n",
      "Episode 11: reward: 195.000, steps: 195\n",
      "Episode 12: reward: 200.000, steps: 200\n",
      "Episode 13: reward: 193.000, steps: 193\n",
      "Episode 14: reward: 200.000, steps: 200\n",
      "Episode 15: reward: 168.000, steps: 168\n",
      "Episode 16: reward: 200.000, steps: 200\n",
      "Episode 17: reward: 178.000, steps: 178\n",
      "Episode 18: reward: 199.000, steps: 199\n",
      "Episode 19: reward: 200.000, steps: 200\n",
      "Episode 20: reward: 197.000, steps: 197\n",
      "Episode 21: reward: 200.000, steps: 200\n",
      "Episode 22: reward: 190.000, steps: 190\n",
      "Episode 23: reward: 200.000, steps: 200\n",
      "Episode 24: reward: 200.000, steps: 200\n",
      "Episode 25: reward: 172.000, steps: 172\n",
      "Episode 26: reward: 200.000, steps: 200\n",
      "Episode 27: reward: 200.000, steps: 200\n",
      "Episode 28: reward: 168.000, steps: 168\n",
      "Episode 29: reward: 200.000, steps: 200\n",
      "Episode 30: reward: 193.000, steps: 193\n",
      "Episode 31: reward: 198.000, steps: 198\n",
      "Episode 32: reward: 200.000, steps: 200\n",
      "Episode 33: reward: 200.000, steps: 200\n",
      "Episode 34: reward: 190.000, steps: 190\n",
      "Episode 35: reward: 158.000, steps: 158\n",
      "Episode 36: reward: 187.000, steps: 187\n",
      "Episode 37: reward: 192.000, steps: 192\n",
      "Episode 38: reward: 172.000, steps: 172\n",
      "Episode 39: reward: 200.000, steps: 200\n",
      "Episode 40: reward: 200.000, steps: 200\n",
      "Episode 41: reward: 200.000, steps: 200\n",
      "Episode 42: reward: 195.000, steps: 195\n",
      "Episode 43: reward: 200.000, steps: 200\n",
      "Episode 44: reward: 198.000, steps: 198\n",
      "Episode 45: reward: 200.000, steps: 200\n",
      "Episode 46: reward: 192.000, steps: 192\n",
      "Episode 47: reward: 200.000, steps: 200\n",
      "Episode 48: reward: 200.000, steps: 200\n",
      "Episode 49: reward: 200.000, steps: 200\n",
      "Episode 50: reward: 182.000, steps: 182\n",
      "Episode 51: reward: 200.000, steps: 200\n",
      "Episode 52: reward: 200.000, steps: 200\n",
      "Episode 53: reward: 188.000, steps: 188\n",
      "Episode 54: reward: 200.000, steps: 200\n",
      "Episode 55: reward: 175.000, steps: 175\n",
      "Episode 56: reward: 178.000, steps: 178\n",
      "Episode 57: reward: 200.000, steps: 200\n",
      "Episode 58: reward: 200.000, steps: 200\n",
      "Episode 59: reward: 184.000, steps: 184\n",
      "Episode 60: reward: 200.000, steps: 200\n",
      "Episode 61: reward: 189.000, steps: 189\n",
      "Episode 62: reward: 200.000, steps: 200\n",
      "Episode 63: reward: 183.000, steps: 183\n",
      "Episode 64: reward: 181.000, steps: 181\n",
      "Episode 65: reward: 181.000, steps: 181\n",
      "Episode 66: reward: 176.000, steps: 176\n",
      "Episode 67: reward: 192.000, steps: 192\n",
      "Episode 68: reward: 187.000, steps: 187\n",
      "Episode 69: reward: 200.000, steps: 200\n",
      "Episode 70: reward: 188.000, steps: 188\n",
      "Episode 71: reward: 200.000, steps: 200\n",
      "Episode 72: reward: 184.000, steps: 184\n",
      "Episode 73: reward: 200.000, steps: 200\n",
      "Episode 74: reward: 191.000, steps: 191\n",
      "Episode 75: reward: 181.000, steps: 181\n",
      "Episode 76: reward: 200.000, steps: 200\n",
      "Episode 77: reward: 178.000, steps: 178\n",
      "Episode 78: reward: 200.000, steps: 200\n",
      "Episode 79: reward: 200.000, steps: 200\n",
      "Episode 80: reward: 163.000, steps: 163\n",
      "Episode 81: reward: 194.000, steps: 194\n",
      "Episode 82: reward: 200.000, steps: 200\n",
      "Episode 83: reward: 186.000, steps: 186\n",
      "Episode 84: reward: 200.000, steps: 200\n",
      "Episode 85: reward: 170.000, steps: 170\n",
      "Episode 86: reward: 200.000, steps: 200\n",
      "Episode 87: reward: 200.000, steps: 200\n",
      "Episode 88: reward: 187.000, steps: 187\n",
      "Episode 89: reward: 173.000, steps: 173\n",
      "Episode 90: reward: 181.000, steps: 181\n",
      "Episode 91: reward: 200.000, steps: 200\n",
      "Episode 92: reward: 200.000, steps: 200\n",
      "Episode 93: reward: 194.000, steps: 194\n",
      "Episode 94: reward: 200.000, steps: 200\n",
      "Episode 95: reward: 179.000, steps: 179\n",
      "Episode 96: reward: 200.000, steps: 200\n",
      "Episode 97: reward: 174.000, steps: 174\n",
      "Episode 98: reward: 178.000, steps: 178\n",
      "Episode 99: reward: 198.000, steps: 198\n",
      "Episode 100: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "190.95"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "np.mean(results.history[\"episode_reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[98.962425 96.43396 ]] 0 False\n",
      "[[99.99229 99.69212]] 0 False\n",
      "[[99.091866 99.88295 ]] 1 False\n",
      "[[100.281   100.25469]] 0 False\n",
      "[[ 99.337814 100.38535 ]] 1 False\n",
      "[[100.52338  100.741745]] 1 False\n",
      "[[101.264656  98.528435]] 0 False\n",
      "[[102.08593 101.90326]] 0 False\n",
      "[[101.2743  102.14484]] 1 False\n",
      "[[102.56575 102.58135]] 1 False\n",
      "[[103.35154 100.07681]] 0 False\n",
      "[[103.92098 103.48895]] 0 False\n",
      "[[102.63896  103.204704]] 1 False\n",
      "[[104.02851  103.723755]] 0 False\n",
      "[[102.29484  102.952415]] 1 False\n",
      "[[103.57693 103.34411]] 0 False\n",
      "[[102.0173  102.72908]] 1 False\n",
      "[[103.32009  103.081245]] 0 False\n",
      "[[101.80416  102.533516]] 1 False\n",
      "[[102.86048  102.449356]] 0 False\n",
      "[[101.65482 102.36526]] 1 False\n",
      "[[102.05963 101.23636]] 0 False\n",
      "[[101.57032 102.22455]] 1 False\n",
      "[[101.187325  99.850525]] 0 False\n",
      "[[101.5533 102.1123]] 1 False\n",
      "[[100.36224  98.25416]] 0 False\n",
      "[[100.855225 100.868576]] 1 False\n",
      "[[99.445305 96.420815]] 0 False\n",
      "[[99.92192 99.17427]] 0 False\n",
      "[[ 98.89958 100.25314]] 1 False\n",
      "[[98.75707 98.38344]] 0 False\n",
      "[[97.79438  99.499954]] 1 False\n",
      "[[97.67323  97.612656]] 0 False\n",
      "[[96.77846  98.786606]] 1 False\n",
      "[[96.65451 96.85074]] 1 False\n",
      "[[96.07169 92.41464]] 0 False\n",
      "[[95.82091 94.93556]] 0 False\n",
      "[[95.12405 96.33801]] 1 False\n",
      "[[94.69427 93.85083]] 0 False\n",
      "[[94.01008 95.22579]] 1 False\n",
      "[[93.66029 92.7857 ]] 0 False\n",
      "[[92.90789 94.04178]] 1 False\n",
      "[[92.634995 91.64417 ]] 0 False\n",
      "[[91.83948  92.807556]] 1 False\n",
      "[[91.58867 90.38888]] 0 False\n",
      "[[90.776375 91.48604 ]] 1 False\n",
      "[[90.50513  88.995346]] 0 False\n",
      "[[89.66739  90.012405]] 1 False\n",
      "[[89.36745 87.4359 ]] 0 False\n",
      "[[88.49504 88.35717]] 0 False\n",
      "[[87.5985  89.50961]] 1 False\n",
      "[[87.30022 87.85363]] 1 False\n",
      "[[87.01437  85.204765]] 0 False\n",
      "[[86.15057 86.0481 ]] 0 False\n",
      "[[85.24739 86.75567]] 1 False\n",
      "[[84.94069 85.07653]] 1 False\n",
      "[[84.642296 82.592705]] 0 False\n",
      "[[83.747406 83.03469 ]] 0 False\n",
      "[[82.80953 83.60197]] 1 False\n",
      "[[82.46097 81.79026]] 0 False\n",
      "[[81.545135 82.35726 ]] 1 False\n",
      "[[81.21203  80.536316]] 0 False\n",
      "[[80.309845 81.092606]] 1 False\n",
      "[[79.984856 79.25326 ]] 0 False\n",
      "[[79.08845  79.788345]] 1 False\n",
      "[[78.764496 77.921   ]] 0 False\n",
      "[[77.86624 78.42405]] 1 False\n",
      "[[77.536354 76.518486]] 0 False\n",
      "[[76.62872 76.97804]] 1 False\n",
      "[[76.28579  75.023155]] 0 False\n",
      "[[75.36114 75.42685]] 1 False\n",
      "[[74.99787 73.41046]] 0 False\n",
      "[[74.04817  73.744606]] 0 False\n",
      "[[72.85683 74.48486]] 1 False\n",
      "[[72.72811  73.001144]] 1 False\n",
      "[[72.40021 70.9804 ]] 0 False\n",
      "[[71.48256 71.30775]] 0 False\n",
      "[[70.28198 72.01104]] 1 False\n",
      "[[70.204445 70.52223 ]] 1 False\n",
      "[[70.70858 69.55054]] 0 False\n",
      "[[69.54856 69.50922]] 0 False\n",
      "[[68.29321 70.07594]] 1 False\n",
      "[[68.87299 69.4187 ]] 1 False\n",
      "[[69.36925 68.38473]] 0 False\n",
      "[[68.198074 68.27525 ]] 1 False\n",
      "[[68.65896  67.153915]] 0 False\n",
      "[[67.4485  66.94696]] 0 False\n",
      "[[66.47725 67.73679]] 1 False\n",
      "[[66.65667 66.48669]] 0 False\n",
      "[[65.6155  67.23884]] 1 False\n",
      "[[65.88152 65.99234]] 1 False\n",
      "[[66.31653 64.73149]] 0 False\n",
      "[[65.07301 64.37113]] 0 False\n",
      "[[64.308464 65.32684 ]] 1 False\n",
      "[[64.17919  63.557987]] 0 False\n",
      "[[63.45671 64.54379]] 1 False\n",
      "[[63.190804 62.61792 ]] 0 False\n",
      "[[62.58709  63.646687]] 1 False\n",
      "[[62.16733  61.557224]] 0 False\n",
      "[[61.604733 62.539474]] 1 False\n",
      "[[61.095615 60.353256]] 0 False\n",
      "[[60.589745 61.294235]] 1 False\n",
      "[[59.96147  58.980045]] 0 False\n",
      "[[59.529263 59.884327]] 1 False\n",
      "[[58.749336 57.407764]] 0 False\n",
      "[[58.409485 58.279312]] 0 False\n",
      "[[56.92833 58.7039 ]] 1 False\n",
      "[[57.156982 57.38165 ]] 1 False\n",
      "[[56.08854  54.538944]] 0 False\n",
      "[[55.996113 55.49406 ]] 0 False\n",
      "[[54.714508 55.672405]] 1 False\n",
      "[[54.671375 54.255547]] 0 False\n",
      "[[53.435436 54.19561 ]] 1 False\n",
      "[[53.346935 52.89807 ]] 0 False\n",
      "[[52.226463 52.675198]] 1 False\n",
      "[[52.053497 51.305046]] 0 False\n",
      "[[51.4062 51.3255]] 0 False\n",
      "[[50.86925  51.983566]] 1 False\n",
      "[[49.981327 50.108814]] 1 False\n",
      "[[49.949493 48.768234]] 0 False\n",
      "[[49.434643 48.796032]] 0 False\n",
      "[[48.986317 49.426678]] 1 False\n",
      "[[48.185432 47.514683]] 0 False\n",
      "[[47.78579 48.15132]] 1 False\n",
      "[[47.031307 46.238018]] 0 False\n",
      "[[46.6768   46.870853]] 1 False\n",
      "[[45.96599  44.947884]] 0 False\n",
      "[[45.654266 45.567215]] 0 False\n",
      "[[44.623096 46.03736 ]] 1 False\n",
      "[[43.93597 44.36006]] 1 False\n",
      "[[43.316105 42.454655]] 0 False\n",
      "[[43.09205  43.089413]] 0 False\n",
      "[[41.91698 43.44173]] 1 False\n",
      "[[41.448284 41.835335]] 1 False\n",
      "[[41.026638 39.989918]] 0 False\n",
      "[[40.63883 40.46859]] 0 False\n",
      "[[39.511898 40.77956 ]] 1 False\n",
      "[[39.113674 39.136723]] 1 False\n",
      "[[38.971672 37.355877]] 0 False\n",
      "[[38.442165 37.670063]] 0 False\n",
      "[[37.3835  37.91586]] 1 False\n",
      "[[37.05464  36.202972]] 0 False\n",
      "[[36.03875  36.436825]] 1 False\n",
      "[[35.751488 34.706802]] 0 False\n",
      "[[34.77632  34.918434]] 1 False\n",
      "[[34.52935  33.162086]] 0 False\n",
      "[[33.593876 33.340763]] 0 False\n",
      "[[32.28887  33.552216]] 1 False\n",
      "[[31.719534 31.825724]] 1 False\n",
      "[[31.564018 30.051115]] 0 False\n",
      "[[30.717571 30.203722]] 0 False\n",
      "[[29.499207 30.378422]] 1 False\n",
      "[[29.014921 28.605684]] 0 False\n",
      "[[27.845747 28.781672]] 1 False\n",
      "[[27.408274 27.002903]] 0 False\n",
      "[[26.284609 27.16956 ]] 1 False\n",
      "[[25.891146 25.375633]] 0 False\n",
      "[[24.810493 25.522266]] 1 False\n",
      "[[24.459368 23.703737]] 0 False\n",
      "[[23.420326 23.819288]] 1 False\n",
      "[[23.110975 21.966291]] 0 False\n",
      "[[22.113203 22.038996]] 0 False\n",
      "[[20.747116 22.138926]] 1 False\n",
      "[[20.092432 20.377642]] 1 False\n",
      "[[19.776257 18.780634]] 0 False\n",
      "[[18.831427 18.92199 ]] 1 False\n",
      "[[18.545425 17.284464]] 0 False\n",
      "[[17.629427 17.374893]] 0 False\n",
      "[[16.36503  17.416998]] 1 False\n",
      "[[15.77561  15.709496]] 0 False\n",
      "[[14.563156 15.73874 ]] 1 False\n",
      "[[14.024527 14.014444]] 0 False\n",
      "[[13.036337 14.15524 ]] 1 False\n",
      "[[12.371207 12.26916 ]] 0 False\n",
      "[[11.759581 12.628963]] 1 False\n",
      "[[10.839805 10.524856]] 0 True\n",
      "176.0\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "action = dqn.model.predict([[[obs]]]).argmax()\n",
    "tt_reward = 0\n",
    "done = False\n",
    "while not done:\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    result = dqn.model.predict([[[next_state]]])\n",
    "    action = result.argmax()\n",
    "    print(result, action, done)\n",
    "    env.render()\n",
    "    tt_reward += reward\n",
    "print(tt_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-19 23:32:14.004118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-19 23:32:14.004189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 2-Cartpole/assets\n"
     ]
    }
   ],
   "source": [
    "dqn.model.save(\"2-Cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneticAlgorithm",
   "language": "python",
   "name": "geneticalgorithm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
