{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://bi3mer.github.io/blog/post_18/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 10,819\n",
      "Trainable params: 10,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(env.action_space.n))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tttienthinh/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.agents import DQNAgent\n",
    "\n",
    "dqn = DQNAgent(\n",
    "    model=model, \n",
    "    nb_actions=env.action_space.n, \n",
    "    memory=SequentialMemory(limit=50000, window_length=1), \n",
    "    nb_steps_warmup=10,\n",
    "    target_model_update=1e-2, \n",
    "    policy=BoltzmannQPolicy()\n",
    ")\n",
    "\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 150000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tttienthinh/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "/home/tttienthinh/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    200/150000: episode: 1, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.069910, mae: 0.804387, mean_q: -0.994688\n",
      "    400/150000: episode: 2, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.012567, mae: 1.743582, mean_q: -2.554291\n",
      "    600/150000: episode: 3, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.025732, mae: 2.846496, mean_q: -4.193378\n",
      "    800/150000: episode: 4, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.057200, mae: 3.982826, mean_q: -5.873297\n",
      "   1000/150000: episode: 5, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.091419, mae: 5.125338, mean_q: -7.548160\n",
      "   1200/150000: episode: 6, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.176833, mae: 6.228465, mean_q: -9.206479\n",
      "   1400/150000: episode: 7, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.236266, mae: 7.306727, mean_q: -10.755772\n",
      "   1600/150000: episode: 8, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.382296, mae: 8.334019, mean_q: -12.274677\n",
      "   1800/150000: episode: 9, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.399624, mae: 9.302969, mean_q: -13.736231\n",
      "   2000/150000: episode: 10, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.490755, mae: 10.297118, mean_q: -15.184639\n",
      "   2200/150000: episode: 11, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.477208, mae: 11.220292, mean_q: -16.609098\n",
      "   2400/150000: episode: 12, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.715126, mae: 12.142217, mean_q: -17.938770\n",
      "   2600/150000: episode: 13, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.849527, mae: 13.042244, mean_q: -19.258512\n",
      "   2800/150000: episode: 14, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 1.197667, mae: 13.831409, mean_q: -20.370968\n",
      "   3000/150000: episode: 15, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.375010, mae: 14.648493, mean_q: -21.570297\n",
      "   3200/150000: episode: 16, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.419737, mae: 15.432792, mean_q: -22.728374\n",
      "   3400/150000: episode: 17, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.012083, mae: 16.182789, mean_q: -23.962671\n",
      "   3600/150000: episode: 18, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 2.418431, mae: 16.918171, mean_q: -24.918615\n",
      "   3800/150000: episode: 19, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 1.479540, mae: 17.626535, mean_q: -26.118450\n",
      "   4000/150000: episode: 20, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 1.779327, mae: 18.355097, mean_q: -27.130997\n",
      "   4200/150000: episode: 21, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.786120, mae: 19.015854, mean_q: -28.113808\n",
      "   4400/150000: episode: 22, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 1.595482, mae: 19.635626, mean_q: -29.057178\n",
      "   4600/150000: episode: 23, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 2.361309, mae: 20.268997, mean_q: -29.928747\n",
      "   4800/150000: episode: 24, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 2.569810, mae: 20.813318, mean_q: -30.673708\n",
      "   5000/150000: episode: 25, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 2.382077, mae: 21.368198, mean_q: -31.615660\n",
      "   5200/150000: episode: 26, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 2.500371, mae: 21.965637, mean_q: -32.517941\n",
      "   5400/150000: episode: 27, duration: 1.329s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.712933, mae: 22.552158, mean_q: -33.340710\n",
      "   5600/150000: episode: 28, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 2.810313, mae: 23.050600, mean_q: -34.027454\n",
      "   5800/150000: episode: 29, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 3.257680, mae: 23.573753, mean_q: -34.848064\n",
      "   6000/150000: episode: 30, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 3.724244, mae: 23.992823, mean_q: -35.429943\n",
      "   6200/150000: episode: 31, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.957587, mae: 24.443104, mean_q: -36.232998\n",
      "   6400/150000: episode: 32, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 2.697155, mae: 24.959145, mean_q: -36.973145\n",
      "   6600/150000: episode: 33, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 3.953396, mae: 25.393557, mean_q: -37.533005\n",
      "   6800/150000: episode: 34, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 3.832479, mae: 25.847876, mean_q: -38.138046\n",
      "   7000/150000: episode: 35, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 3.567565, mae: 26.215721, mean_q: -38.777660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7200/150000: episode: 36, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 4.250932, mae: 26.688129, mean_q: -39.531445\n",
      "   7400/150000: episode: 37, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.937739, mae: 27.099297, mean_q: -40.117085\n",
      "   7600/150000: episode: 38, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 3.950129, mae: 27.450401, mean_q: -40.555809\n",
      "   7800/150000: episode: 39, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 3.305066, mae: 27.903048, mean_q: -41.356415\n",
      "   8000/150000: episode: 40, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 3.808476, mae: 28.322529, mean_q: -41.930050\n",
      "   8200/150000: episode: 41, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 4.601645, mae: 28.744545, mean_q: -42.580231\n",
      "   8400/150000: episode: 42, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 5.089758, mae: 29.083565, mean_q: -43.013329\n",
      "   8600/150000: episode: 43, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 4.543319, mae: 29.443769, mean_q: -43.566471\n",
      "   8800/150000: episode: 44, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 3.855151, mae: 29.728809, mean_q: -44.086197\n",
      "   9000/150000: episode: 45, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 4.174692, mae: 30.147034, mean_q: -44.689213\n",
      "   9200/150000: episode: 46, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 4.955883, mae: 30.473686, mean_q: -45.149487\n",
      "   9400/150000: episode: 47, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 5.087934, mae: 30.794355, mean_q: -45.604923\n",
      "   9600/150000: episode: 48, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 4.461004, mae: 31.057627, mean_q: -45.940693\n",
      "   9800/150000: episode: 49, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 5.427499, mae: 31.362238, mean_q: -46.318916\n",
      "  10000/150000: episode: 50, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 4.550709, mae: 31.548058, mean_q: -46.732277\n",
      "  10200/150000: episode: 51, duration: 1.575s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 2.880004, mae: 31.831270, mean_q: -47.364639\n",
      "  10400/150000: episode: 52, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 5.278852, mae: 32.184219, mean_q: -47.596996\n",
      "  10600/150000: episode: 53, duration: 1.712s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 4.566450, mae: 32.453773, mean_q: -48.061710\n",
      "  10800/150000: episode: 54, duration: 1.633s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 6.161200, mae: 32.674725, mean_q: -48.347450\n",
      "  11000/150000: episode: 55, duration: 1.653s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.655485, mae: 32.885548, mean_q: -48.626572\n",
      "  11200/150000: episode: 56, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 5.581527, mae: 33.043289, mean_q: -48.982113\n",
      "  11400/150000: episode: 57, duration: 1.722s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 5.681463, mae: 33.225609, mean_q: -49.257587\n",
      "  11600/150000: episode: 58, duration: 2.109s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 5.073795, mae: 33.481476, mean_q: -49.601830\n",
      "  11800/150000: episode: 59, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 5.931932, mae: 33.624062, mean_q: -49.779068\n",
      "  12000/150000: episode: 60, duration: 2.010s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 6.727425, mae: 33.724991, mean_q: -49.814960\n",
      "  12200/150000: episode: 61, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 6.538425, mae: 33.806122, mean_q: -49.888065\n",
      "  12400/150000: episode: 62, duration: 1.979s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 5.327307, mae: 33.872963, mean_q: -50.096485\n",
      "  12600/150000: episode: 63, duration: 1.922s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 6.325846, mae: 34.057693, mean_q: -50.400997\n",
      "  12800/150000: episode: 64, duration: 1.972s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 4.218305, mae: 34.191158, mean_q: -50.819794\n",
      "  13000/150000: episode: 65, duration: 2.029s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.133628, mae: 34.473732, mean_q: -50.988701\n",
      "  13200/150000: episode: 66, duration: 1.990s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 5.702402, mae: 34.699093, mean_q: -51.452492\n",
      "  13400/150000: episode: 67, duration: 1.925s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 6.643838, mae: 34.843746, mean_q: -51.606167\n",
      "  13600/150000: episode: 68, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 6.759688, mae: 34.894676, mean_q: -51.577255\n",
      "  13800/150000: episode: 69, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.347086, mae: 34.980640, mean_q: -51.810284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14000/150000: episode: 70, duration: 1.928s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 4.693427, mae: 35.217049, mean_q: -52.313652\n",
      "  14200/150000: episode: 71, duration: 1.945s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 7.421144, mae: 35.383717, mean_q: -52.398514\n",
      "  14400/150000: episode: 72, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 5.477354, mae: 35.538548, mean_q: -52.675816\n",
      "  14600/150000: episode: 73, duration: 1.945s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 7.124365, mae: 35.651695, mean_q: -52.805889\n",
      "  14800/150000: episode: 74, duration: 2.014s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.022964, mae: 35.871838, mean_q: -53.149590\n",
      "  15000/150000: episode: 75, duration: 1.978s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 5.320334, mae: 36.038998, mean_q: -53.388683\n",
      "  15200/150000: episode: 76, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 5.875841, mae: 36.276684, mean_q: -53.908279\n",
      "  15400/150000: episode: 77, duration: 1.955s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 6.318652, mae: 36.539459, mean_q: -54.110508\n",
      "  15600/150000: episode: 78, duration: 2.071s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 7.300470, mae: 36.715668, mean_q: -54.276474\n",
      "  15800/150000: episode: 79, duration: 1.935s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 8.691154, mae: 36.826370, mean_q: -54.413639\n",
      "  16000/150000: episode: 80, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 6.336411, mae: 36.937344, mean_q: -54.775829\n",
      "  16200/150000: episode: 81, duration: 2.123s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.062340, mae: 37.073524, mean_q: -54.847313\n",
      "  16400/150000: episode: 82, duration: 2.052s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 6.793336, mae: 37.083527, mean_q: -54.945488\n",
      "  16600/150000: episode: 83, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.108396, mae: 37.201538, mean_q: -55.141445\n",
      "  16800/150000: episode: 84, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 5.748633, mae: 37.350826, mean_q: -55.448963\n",
      "  17000/150000: episode: 85, duration: 1.981s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 9.323524, mae: 37.408459, mean_q: -55.270477\n",
      "  17200/150000: episode: 86, duration: 1.928s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.367958, mae: 37.439087, mean_q: -55.528759\n",
      "  17400/150000: episode: 87, duration: 2.001s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 7.406763, mae: 37.526619, mean_q: -55.628021\n",
      "  17600/150000: episode: 88, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.378608, mae: 37.684223, mean_q: -55.835609\n",
      "  17800/150000: episode: 89, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 6.396463, mae: 37.836868, mean_q: -56.154617\n",
      "  18000/150000: episode: 90, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 6.348165, mae: 38.010059, mean_q: -56.498104\n",
      "  18200/150000: episode: 91, duration: 1.884s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.488524, mae: 38.151047, mean_q: -56.613998\n",
      "  18400/150000: episode: 92, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 7.207521, mae: 38.314396, mean_q: -56.955196\n",
      "  18600/150000: episode: 93, duration: 2.107s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 8.095067, mae: 38.459606, mean_q: -56.958427\n",
      "  18800/150000: episode: 94, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 8.374751, mae: 38.555389, mean_q: -57.040844\n",
      "  19000/150000: episode: 95, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 6.875562, mae: 38.711197, mean_q: -57.499298\n",
      "  19200/150000: episode: 96, duration: 1.985s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 7.207747, mae: 38.790993, mean_q: -57.506107\n",
      "  19400/150000: episode: 97, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.474333, mae: 38.750011, mean_q: -57.422100\n",
      "  19600/150000: episode: 98, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 9.243856, mae: 38.765141, mean_q: -57.369507\n",
      "  19800/150000: episode: 99, duration: 2.077s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 6.362975, mae: 38.790279, mean_q: -57.635483\n",
      "  20000/150000: episode: 100, duration: 1.940s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 9.922456, mae: 38.912716, mean_q: -57.629318\n",
      "  20200/150000: episode: 101, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 6.716504, mae: 38.940540, mean_q: -57.850597\n",
      "  20400/150000: episode: 102, duration: 1.788s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 6.816788, mae: 39.078575, mean_q: -58.017696\n",
      "  20600/150000: episode: 103, duration: 1.929s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.889559, mae: 39.169640, mean_q: -57.980801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20800/150000: episode: 104, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 9.403881, mae: 39.088802, mean_q: -57.776768\n",
      "  21000/150000: episode: 105, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 7.847235, mae: 39.129921, mean_q: -57.987656\n",
      "  21200/150000: episode: 106, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 10.902219, mae: 39.077709, mean_q: -57.683800\n",
      "  21400/150000: episode: 107, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 7.019603, mae: 39.000702, mean_q: -57.860695\n",
      "  21600/150000: episode: 108, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 9.369748, mae: 39.215630, mean_q: -58.105125\n",
      "  21800/150000: episode: 109, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.498436, mae: 39.267921, mean_q: -58.178848\n",
      "  22000/150000: episode: 110, duration: 1.754s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 9.510682, mae: 39.302197, mean_q: -58.245392\n",
      "  22200/150000: episode: 111, duration: 1.768s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 5.800253, mae: 39.391232, mean_q: -58.507172\n",
      "  22400/150000: episode: 112, duration: 1.806s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 8.611917, mae: 39.508026, mean_q: -58.501537\n",
      "  22600/150000: episode: 113, duration: 1.765s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.020341, mae: 39.585861, mean_q: -58.735680\n",
      "  22800/150000: episode: 114, duration: 1.774s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 8.106196, mae: 39.684711, mean_q: -58.833813\n",
      "  23000/150000: episode: 115, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 12.198825, mae: 39.650665, mean_q: -58.580746\n",
      "  23200/150000: episode: 116, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 10.564457, mae: 39.633255, mean_q: -58.614079\n",
      "  23400/150000: episode: 117, duration: 2.060s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 10.088096, mae: 39.514881, mean_q: -58.451084\n",
      "  23600/150000: episode: 118, duration: 1.996s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 5.667644, mae: 39.670902, mean_q: -59.002735\n",
      "  23800/150000: episode: 119, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 9.837588, mae: 39.754845, mean_q: -58.773792\n",
      "  24000/150000: episode: 120, duration: 1.802s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 7.027730, mae: 39.832813, mean_q: -59.132511\n",
      "  24200/150000: episode: 121, duration: 1.805s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.060607, mae: 39.796230, mean_q: -58.871338\n",
      "  24400/150000: episode: 122, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 7.285475, mae: 39.822140, mean_q: -59.095245\n",
      "  24600/150000: episode: 123, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 8.614057, mae: 39.843548, mean_q: -59.060577\n",
      "  24800/150000: episode: 124, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 7.613134, mae: 39.956345, mean_q: -59.229580\n",
      "  25000/150000: episode: 125, duration: 1.800s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.323335, mae: 39.969662, mean_q: -59.105717\n",
      "  25200/150000: episode: 126, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.217037, mae: 39.936142, mean_q: -59.248009\n",
      "  25400/150000: episode: 127, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.392544, mae: 39.965714, mean_q: -59.247646\n",
      "  25600/150000: episode: 128, duration: 1.974s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 5.760690, mae: 40.095680, mean_q: -59.627529\n",
      "  25800/150000: episode: 129, duration: 2.014s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.855426, mae: 40.351810, mean_q: -59.710800\n",
      "  26000/150000: episode: 130, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.304710, mae: 40.307247, mean_q: -59.808266\n",
      "  26200/150000: episode: 131, duration: 1.943s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.875463, mae: 40.327126, mean_q: -59.620640\n",
      "  26400/150000: episode: 132, duration: 2.394s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.378906, mae: 40.400017, mean_q: -59.937325\n",
      "  26600/150000: episode: 133, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.947611, mae: 40.405361, mean_q: -59.912529\n",
      "  26800/150000: episode: 134, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.996579, mae: 40.472527, mean_q: -60.026249\n",
      "  27000/150000: episode: 135, duration: 2.086s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.492183, mae: 40.520550, mean_q: -59.995514\n",
      "  27200/150000: episode: 136, duration: 2.079s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 9.629940, mae: 40.381069, mean_q: -59.800873\n",
      "  27400/150000: episode: 137, duration: 2.414s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 9.425563, mae: 40.420494, mean_q: -59.908268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  27600/150000: episode: 138, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.565435, mae: 40.452793, mean_q: -60.042198\n",
      "  27800/150000: episode: 139, duration: 2.011s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 10.492897, mae: 40.455051, mean_q: -59.954678\n",
      "  28000/150000: episode: 140, duration: 1.950s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 6.526506, mae: 40.610634, mean_q: -60.322617\n",
      "  28200/150000: episode: 141, duration: 1.960s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 10.573242, mae: 40.655262, mean_q: -60.119755\n",
      "  28400/150000: episode: 142, duration: 2.023s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 7.263603, mae: 40.545456, mean_q: -60.269073\n",
      "  28600/150000: episode: 143, duration: 1.929s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 6.494232, mae: 40.634327, mean_q: -60.360645\n",
      "  28800/150000: episode: 144, duration: 2.003s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.136594, mae: 40.735806, mean_q: -60.406284\n",
      "  29000/150000: episode: 145, duration: 1.994s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 10.199188, mae: 40.745018, mean_q: -60.330864\n",
      "  29200/150000: episode: 146, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.193198, mae: 40.789093, mean_q: -60.480175\n",
      "  29400/150000: episode: 147, duration: 1.968s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 11.103637, mae: 40.813869, mean_q: -60.402832\n",
      "  29600/150000: episode: 148, duration: 2.020s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.973345, mae: 40.751808, mean_q: -60.376339\n",
      "  29800/150000: episode: 149, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 7.881754, mae: 40.919140, mean_q: -60.709816\n",
      "  30000/150000: episode: 150, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 9.069616, mae: 40.860569, mean_q: -60.544277\n",
      "  30200/150000: episode: 151, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 9.561937, mae: 40.924953, mean_q: -60.614426\n",
      "  30400/150000: episode: 152, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.015512, mae: 40.849403, mean_q: -60.635883\n",
      "  30600/150000: episode: 153, duration: 2.010s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 5.748627, mae: 40.952316, mean_q: -60.921017\n",
      "  30800/150000: episode: 154, duration: 2.009s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 8.646556, mae: 41.039448, mean_q: -60.891483\n",
      "  31000/150000: episode: 155, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 10.258300, mae: 41.074169, mean_q: -60.779575\n",
      "  31200/150000: episode: 156, duration: 2.421s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 13.363352, mae: 41.044815, mean_q: -60.580177\n",
      "  31400/150000: episode: 157, duration: 3.097s, episode steps: 200, steps per second:  65, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 5.460211, mae: 41.011009, mean_q: -60.966415\n",
      "  31600/150000: episode: 158, duration: 2.727s, episode steps: 200, steps per second:  73, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 10.444344, mae: 41.145943, mean_q: -60.968773\n",
      "  31800/150000: episode: 159, duration: 2.301s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 9.115100, mae: 41.111542, mean_q: -61.039806\n",
      "  32000/150000: episode: 160, duration: 2.387s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 7.221740, mae: 41.116581, mean_q: -61.123119\n",
      "  32200/150000: episode: 161, duration: 2.027s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 4.674744, mae: 41.281456, mean_q: -61.385738\n",
      "  32400/150000: episode: 162, duration: 2.018s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 9.609043, mae: 41.399445, mean_q: -61.391132\n",
      "  32600/150000: episode: 163, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 9.006224, mae: 41.475517, mean_q: -61.417919\n",
      "  32800/150000: episode: 164, duration: 2.010s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.201956, mae: 41.430164, mean_q: -61.276913\n",
      "  33000/150000: episode: 165, duration: 2.085s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 10.241900, mae: 41.415272, mean_q: -61.312401\n",
      "  33200/150000: episode: 166, duration: 2.488s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 6.508691, mae: 41.502018, mean_q: -61.738781\n",
      "  33400/150000: episode: 167, duration: 1.985s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.220400, mae: 41.584328, mean_q: -61.756073\n",
      "  33600/150000: episode: 168, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 9.459446, mae: 41.610615, mean_q: -61.664745\n",
      "  33800/150000: episode: 169, duration: 2.084s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 8.018139, mae: 41.609852, mean_q: -61.686554\n",
      "  34000/150000: episode: 170, duration: 1.951s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 10.132805, mae: 41.547062, mean_q: -61.506500\n",
      "  34200/150000: episode: 171, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 7.619855, mae: 41.573048, mean_q: -61.722889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  34400/150000: episode: 172, duration: 1.959s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.791001, mae: 41.648418, mean_q: -61.924911\n",
      "  34600/150000: episode: 173, duration: 1.962s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 6.834288, mae: 41.872997, mean_q: -62.198589\n",
      "  34800/150000: episode: 174, duration: 1.953s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 8.412276, mae: 42.006851, mean_q: -62.392967\n",
      "  35000/150000: episode: 175, duration: 2.026s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 13.170584, mae: 41.950665, mean_q: -61.975292\n",
      "  35200/150000: episode: 176, duration: 1.969s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 7.045178, mae: 41.787201, mean_q: -61.960167\n",
      "  35400/150000: episode: 177, duration: 2.067s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.566242, mae: 41.813778, mean_q: -61.888500\n",
      "  35600/150000: episode: 178, duration: 2.046s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.242807, mae: 41.851730, mean_q: -61.965366\n",
      "  35800/150000: episode: 179, duration: 2.005s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.623415, mae: 41.833778, mean_q: -62.022049\n",
      "  36000/150000: episode: 180, duration: 2.016s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 10.807138, mae: 41.899532, mean_q: -61.922588\n",
      "  36200/150000: episode: 181, duration: 1.957s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.273753, mae: 41.829784, mean_q: -61.928944\n",
      "  36400/150000: episode: 182, duration: 2.039s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.730330, mae: 41.843380, mean_q: -62.102890\n",
      "  36600/150000: episode: 183, duration: 2.027s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.352713, mae: 41.926571, mean_q: -62.150352\n",
      "  36800/150000: episode: 184, duration: 2.028s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 11.893874, mae: 41.916115, mean_q: -62.028648\n",
      "  37000/150000: episode: 185, duration: 2.069s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.552635, mae: 41.831070, mean_q: -61.974178\n",
      "  37200/150000: episode: 186, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.873062, mae: 41.971188, mean_q: -62.414913\n",
      "  37400/150000: episode: 187, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 7.423495, mae: 42.068653, mean_q: -62.455643\n",
      "  37600/150000: episode: 188, duration: 2.047s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 11.724140, mae: 41.983612, mean_q: -62.135998\n",
      "  37800/150000: episode: 189, duration: 2.017s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.471892, mae: 41.937775, mean_q: -62.147545\n",
      "  38000/150000: episode: 190, duration: 1.977s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 9.716196, mae: 41.879940, mean_q: -62.126064\n",
      "  38200/150000: episode: 191, duration: 2.061s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 9.702482, mae: 41.865704, mean_q: -61.984638\n",
      "  38400/150000: episode: 192, duration: 2.754s, episode steps: 200, steps per second:  73, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 9.577429, mae: 41.904472, mean_q: -62.077782\n",
      "  38600/150000: episode: 193, duration: 3.392s, episode steps: 200, steps per second:  59, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 10.630221, mae: 41.807560, mean_q: -61.946213\n",
      "  38800/150000: episode: 194, duration: 3.241s, episode steps: 200, steps per second:  62, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 6.921779, mae: 41.882549, mean_q: -62.249561\n",
      "  39000/150000: episode: 195, duration: 3.220s, episode steps: 200, steps per second:  62, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 11.058549, mae: 41.893742, mean_q: -62.037689\n",
      "  39200/150000: episode: 196, duration: 2.862s, episode steps: 200, steps per second:  70, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 10.844770, mae: 41.859013, mean_q: -62.032127\n",
      "  39400/150000: episode: 197, duration: 2.506s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.670303, mae: 41.865112, mean_q: -62.168243\n",
      "  39600/150000: episode: 198, duration: 2.251s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 7.552290, mae: 41.892021, mean_q: -62.133965\n",
      "  39800/150000: episode: 199, duration: 2.231s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.296106, mae: 41.953339, mean_q: -62.181049\n",
      "  40000/150000: episode: 200, duration: 2.433s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.804668, mae: 41.901817, mean_q: -62.171852\n",
      "  40200/150000: episode: 201, duration: 2.115s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.323736, mae: 41.880997, mean_q: -62.186523\n",
      "  40400/150000: episode: 202, duration: 2.140s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 9.850292, mae: 42.011333, mean_q: -62.246193\n",
      "  40600/150000: episode: 203, duration: 2.291s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.915095, mae: 41.972244, mean_q: -62.141582\n",
      "  40800/150000: episode: 204, duration: 2.230s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 11.541202, mae: 41.850029, mean_q: -61.982861\n",
      "  41000/150000: episode: 205, duration: 2.217s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 9.506578, mae: 41.847401, mean_q: -62.054520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  41200/150000: episode: 206, duration: 2.208s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.284353, mae: 41.906727, mean_q: -62.225292\n",
      "  41400/150000: episode: 207, duration: 2.123s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 10.326740, mae: 41.873096, mean_q: -61.965878\n",
      "  41600/150000: episode: 208, duration: 2.251s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.155169, mae: 41.733654, mean_q: -61.848507\n",
      "  41800/150000: episode: 209, duration: 2.202s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.180951, mae: 41.730968, mean_q: -61.933090\n",
      "  42000/150000: episode: 210, duration: 2.253s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 7.399553, mae: 41.885551, mean_q: -62.189198\n",
      "  42200/150000: episode: 211, duration: 2.114s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.756897, mae: 41.897373, mean_q: -62.019707\n",
      "  42400/150000: episode: 212, duration: 2.137s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 8.723475, mae: 41.929825, mean_q: -62.152889\n",
      "  42600/150000: episode: 213, duration: 2.209s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.259853, mae: 41.860706, mean_q: -62.067902\n",
      "  42800/150000: episode: 214, duration: 2.225s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 9.705229, mae: 41.844570, mean_q: -62.026310\n",
      "  43000/150000: episode: 215, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.267345, mae: 41.846054, mean_q: -62.124252\n",
      "  43200/150000: episode: 216, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 9.710904, mae: 41.780331, mean_q: -61.966183\n",
      "  43400/150000: episode: 217, duration: 2.211s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 8.542982, mae: 41.713425, mean_q: -61.852291\n",
      "  43600/150000: episode: 218, duration: 2.164s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 10.415654, mae: 41.651211, mean_q: -61.761112\n",
      "  43800/150000: episode: 219, duration: 2.225s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 6.428549, mae: 41.664074, mean_q: -61.938194\n",
      "  44000/150000: episode: 220, duration: 2.408s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.661291, mae: 41.735737, mean_q: -61.829052\n",
      "  44200/150000: episode: 221, duration: 2.763s, episode steps: 200, steps per second:  72, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 8.184295, mae: 41.714157, mean_q: -61.699268\n",
      "  44400/150000: episode: 222, duration: 2.987s, episode steps: 200, steps per second:  67, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 10.101368, mae: 41.679855, mean_q: -61.738071\n",
      "  44600/150000: episode: 223, duration: 2.877s, episode steps: 200, steps per second:  70, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 10.234741, mae: 41.490635, mean_q: -61.473251\n",
      "  44800/150000: episode: 224, duration: 2.459s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 8.654573, mae: 41.484531, mean_q: -61.579327\n",
      "  45000/150000: episode: 225, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 8.308475, mae: 41.546349, mean_q: -61.615910\n",
      "  45200/150000: episode: 226, duration: 2.243s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 11.561625, mae: 41.446205, mean_q: -61.203869\n",
      "  45400/150000: episode: 227, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 7.828302, mae: 41.476944, mean_q: -61.547684\n",
      "  45600/150000: episode: 228, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.755783, mae: 41.576534, mean_q: -61.742638\n",
      "  45800/150000: episode: 229, duration: 2.153s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 8.412510, mae: 41.529537, mean_q: -61.652443\n",
      "  46000/150000: episode: 230, duration: 2.037s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 8.377456, mae: 41.567204, mean_q: -61.566750\n",
      "  46200/150000: episode: 231, duration: 2.087s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.644815, mae: 41.590851, mean_q: -61.627304\n",
      "  46400/150000: episode: 232, duration: 2.140s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 10.101827, mae: 41.605820, mean_q: -61.606152\n",
      "  46600/150000: episode: 233, duration: 2.164s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 9.226828, mae: 41.559006, mean_q: -61.574150\n",
      "  46800/150000: episode: 234, duration: 2.120s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 11.420274, mae: 41.521500, mean_q: -61.393047\n",
      "  47000/150000: episode: 235, duration: 2.112s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 6.973716, mae: 41.400879, mean_q: -61.470703\n",
      "  47200/150000: episode: 236, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 11.716449, mae: 41.380878, mean_q: -61.222794\n",
      "  47400/150000: episode: 237, duration: 2.049s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 10.166027, mae: 41.367706, mean_q: -61.228016\n",
      "  47600/150000: episode: 238, duration: 2.226s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 8.222376, mae: 41.204193, mean_q: -60.972294\n",
      "  47800/150000: episode: 239, duration: 2.324s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 8.890872, mae: 41.019146, mean_q: -60.748798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  48000/150000: episode: 240, duration: 2.219s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 8.670348, mae: 40.990143, mean_q: -60.771217\n",
      "  48200/150000: episode: 241, duration: 2.206s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 11.419734, mae: 40.925201, mean_q: -60.493927\n",
      "  48400/150000: episode: 242, duration: 2.147s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 9.410836, mae: 40.762085, mean_q: -60.440289\n",
      "  48600/150000: episode: 243, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.089491, mae: 40.711414, mean_q: -60.367393\n",
      "  48800/150000: episode: 244, duration: 2.179s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 9.061184, mae: 40.603409, mean_q: -60.209755\n",
      "  49000/150000: episode: 245, duration: 2.217s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 7.524675, mae: 40.696053, mean_q: -60.352421\n",
      "  49200/150000: episode: 246, duration: 2.214s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 9.655791, mae: 40.630444, mean_q: -60.159042\n",
      "  49400/150000: episode: 247, duration: 2.270s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 11.392004, mae: 40.524719, mean_q: -59.762032\n",
      "  49600/150000: episode: 248, duration: 2.758s, episode steps: 200, steps per second:  73, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 7.930463, mae: 40.535492, mean_q: -60.116329\n",
      "  49800/150000: episode: 249, duration: 2.430s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 9.453774, mae: 40.224987, mean_q: -59.474045\n",
      "  50000/150000: episode: 250, duration: 2.294s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 9.238254, mae: 40.266617, mean_q: -59.656235\n",
      "  50200/150000: episode: 251, duration: 2.319s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 8.765107, mae: 40.236992, mean_q: -59.603348\n",
      "  50400/150000: episode: 252, duration: 2.494s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 10.281173, mae: 40.258152, mean_q: -59.527950\n",
      "  50600/150000: episode: 253, duration: 2.407s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 9.118056, mae: 40.289997, mean_q: -59.691875\n",
      "  50800/150000: episode: 254, duration: 2.338s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 11.027691, mae: 40.080006, mean_q: -59.289661\n",
      "  51000/150000: episode: 255, duration: 2.241s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 10.920715, mae: 39.954521, mean_q: -59.071457\n",
      "  51200/150000: episode: 256, duration: 2.285s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 8.517023, mae: 39.796486, mean_q: -58.965595\n",
      "  51400/150000: episode: 257, duration: 2.275s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 6.958224, mae: 39.830688, mean_q: -59.200947\n",
      "  51600/150000: episode: 258, duration: 2.304s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 8.815891, mae: 39.944912, mean_q: -59.177841\n",
      "  51800/150000: episode: 259, duration: 2.445s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 8.968611, mae: 40.001591, mean_q: -59.399490\n",
      "  52000/150000: episode: 260, duration: 2.451s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 10.288136, mae: 39.951363, mean_q: -59.086456\n",
      "  52200/150000: episode: 261, duration: 2.257s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 10.406433, mae: 39.810268, mean_q: -58.911957\n",
      "  52400/150000: episode: 262, duration: 2.312s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 9.173988, mae: 39.724072, mean_q: -58.803513\n",
      "  52600/150000: episode: 263, duration: 2.401s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 7.728115, mae: 39.764236, mean_q: -58.951885\n",
      "  52800/150000: episode: 264, duration: 2.389s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.488707, mae: 39.748341, mean_q: -58.859600\n",
      "  53000/150000: episode: 265, duration: 2.270s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 7.662096, mae: 39.738007, mean_q: -59.002598\n",
      "  53200/150000: episode: 266, duration: 2.357s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 9.843828, mae: 39.849064, mean_q: -59.069550\n",
      "  53400/150000: episode: 267, duration: 2.328s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 8.001419, mae: 39.856445, mean_q: -59.168877\n",
      "  53600/150000: episode: 268, duration: 2.518s, episode steps: 200, steps per second:  79, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 9.485464, mae: 39.815922, mean_q: -58.909271\n",
      "  53800/150000: episode: 269, duration: 2.503s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.012074, mae: 39.897697, mean_q: -59.106266\n",
      "  54000/150000: episode: 270, duration: 2.377s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 6.583680, mae: 39.948765, mean_q: -59.270824\n",
      "  54200/150000: episode: 271, duration: 2.292s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 8.983088, mae: 40.027382, mean_q: -59.298080\n",
      "  54400/150000: episode: 272, duration: 2.343s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 10.644910, mae: 39.792648, mean_q: -58.904942\n",
      "  54600/150000: episode: 273, duration: 2.343s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 4.566061, mae: 39.961166, mean_q: -59.388309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  54800/150000: episode: 274, duration: 2.239s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.973886, mae: 39.968472, mean_q: -59.242989\n",
      "  55000/150000: episode: 275, duration: 2.242s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 7.536655, mae: 39.868748, mean_q: -59.173634\n",
      "  55200/150000: episode: 276, duration: 2.242s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 7.199480, mae: 39.977833, mean_q: -59.383984\n",
      "  55400/150000: episode: 277, duration: 2.224s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.508230, mae: 40.113705, mean_q: -59.444317\n",
      "  55600/150000: episode: 278, duration: 2.553s, episode steps: 200, steps per second:  78, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 10.760957, mae: 40.023117, mean_q: -59.134979\n",
      "  55800/150000: episode: 279, duration: 2.515s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 9.160686, mae: 39.979645, mean_q: -59.228397\n",
      "  56000/150000: episode: 280, duration: 2.326s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 9.371096, mae: 40.004261, mean_q: -59.258965\n",
      "  56200/150000: episode: 281, duration: 2.292s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.636374, mae: 40.060986, mean_q: -59.403809\n",
      "  56400/150000: episode: 282, duration: 2.402s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 8.871839, mae: 40.103634, mean_q: -59.371449\n",
      "  56600/150000: episode: 283, duration: 2.218s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 8.494833, mae: 40.106094, mean_q: -59.494423\n",
      "  56800/150000: episode: 284, duration: 2.244s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 8.294305, mae: 40.092304, mean_q: -59.416779\n",
      "  57000/150000: episode: 285, duration: 2.305s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 6.757316, mae: 40.087116, mean_q: -59.535294\n",
      "  57200/150000: episode: 286, duration: 2.286s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.325664, mae: 40.193817, mean_q: -59.708672\n",
      "  57400/150000: episode: 287, duration: 2.271s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 9.118176, mae: 40.162479, mean_q: -59.384804\n",
      "  57600/150000: episode: 288, duration: 2.326s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 8.763885, mae: 40.012924, mean_q: -59.210648\n",
      "  57800/150000: episode: 289, duration: 2.262s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 8.090770, mae: 39.970150, mean_q: -59.291454\n",
      "  58000/150000: episode: 290, duration: 2.276s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.944890, mae: 40.068283, mean_q: -59.326817\n",
      "  58200/150000: episode: 291, duration: 2.277s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.635817, mae: 40.151356, mean_q: -59.574764\n",
      "  58400/150000: episode: 292, duration: 2.385s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 12.085086, mae: 40.059666, mean_q: -59.100899\n",
      "  58600/150000: episode: 293, duration: 2.278s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 10.219483, mae: 39.989109, mean_q: -59.272324\n",
      "  58800/150000: episode: 294, duration: 2.331s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.683697, mae: 40.028587, mean_q: -59.340782\n",
      "  59000/150000: episode: 295, duration: 2.207s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 9.178659, mae: 40.071667, mean_q: -59.391701\n",
      "  59200/150000: episode: 296, duration: 2.325s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 10.427857, mae: 40.076412, mean_q: -59.349403\n",
      "  59400/150000: episode: 297, duration: 2.241s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 9.396426, mae: 40.064102, mean_q: -59.270805\n",
      "  59600/150000: episode: 298, duration: 2.261s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 9.835506, mae: 39.980198, mean_q: -59.208885\n",
      "  59800/150000: episode: 299, duration: 2.354s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 8.117915, mae: 39.877773, mean_q: -59.114258\n",
      "  60000/150000: episode: 300, duration: 2.394s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 8.455313, mae: 39.823940, mean_q: -59.002216\n",
      "  60200/150000: episode: 301, duration: 2.253s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 8.883981, mae: 39.843250, mean_q: -59.062744\n",
      "  60400/150000: episode: 302, duration: 2.275s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.279286, mae: 39.786762, mean_q: -59.005707\n",
      "  60600/150000: episode: 303, duration: 2.295s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 8.323608, mae: 39.827587, mean_q: -58.983170\n",
      "  60800/150000: episode: 304, duration: 2.362s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.049444, mae: 39.972935, mean_q: -59.385094\n",
      "  61000/150000: episode: 305, duration: 2.300s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 8.831570, mae: 40.031754, mean_q: -59.337772\n",
      "  61200/150000: episode: 306, duration: 2.258s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 6.676416, mae: 40.101292, mean_q: -59.478580\n",
      "  61400/150000: episode: 307, duration: 2.309s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.850694, mae: 40.195175, mean_q: -59.630703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  61600/150000: episode: 308, duration: 2.298s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 9.530616, mae: 40.186520, mean_q: -59.561241\n",
      "  61800/150000: episode: 309, duration: 2.342s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.236637, mae: 40.301826, mean_q: -59.792305\n",
      "  62000/150000: episode: 310, duration: 2.175s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 10.170484, mae: 40.248016, mean_q: -59.508842\n",
      "  62200/150000: episode: 311, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.725368, mae: 40.309608, mean_q: -59.717850\n",
      "  62400/150000: episode: 312, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 9.126795, mae: 40.241013, mean_q: -59.536747\n",
      "  62600/150000: episode: 313, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 10.673794, mae: 40.188629, mean_q: -59.585655\n",
      "  62800/150000: episode: 314, duration: 1.995s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.764446, mae: 40.022377, mean_q: -59.222305\n",
      "  63000/150000: episode: 315, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.748777, mae: 39.991119, mean_q: -59.286564\n",
      "  63200/150000: episode: 316, duration: 2.109s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 8.569777, mae: 39.896816, mean_q: -59.133785\n",
      "  63400/150000: episode: 317, duration: 2.043s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.543365, mae: 39.865288, mean_q: -59.100143\n",
      "  63600/150000: episode: 318, duration: 1.982s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 6.982439, mae: 39.957573, mean_q: -59.314442\n",
      "  63800/150000: episode: 319, duration: 1.972s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 9.167027, mae: 39.993713, mean_q: -59.177582\n",
      "  64000/150000: episode: 320, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 10.281447, mae: 39.889877, mean_q: -59.077835\n",
      "  64200/150000: episode: 321, duration: 1.960s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 11.714092, mae: 39.862965, mean_q: -58.938820\n",
      "  64400/150000: episode: 322, duration: 1.995s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.392059, mae: 39.723415, mean_q: -58.782791\n",
      "  64600/150000: episode: 323, duration: 2.007s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.175498, mae: 39.753517, mean_q: -59.002323\n",
      "  64800/150000: episode: 324, duration: 1.928s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 10.624372, mae: 39.732170, mean_q: -58.735577\n",
      "  65000/150000: episode: 325, duration: 2.064s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 5.690382, mae: 39.716663, mean_q: -59.017746\n",
      "  65200/150000: episode: 326, duration: 1.984s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 8.929535, mae: 39.788349, mean_q: -58.903233\n",
      "  65400/150000: episode: 327, duration: 2.019s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.142142, mae: 39.652035, mean_q: -58.681442\n",
      "  65600/150000: episode: 328, duration: 2.032s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.537911, mae: 39.654728, mean_q: -58.763195\n",
      "  65800/150000: episode: 329, duration: 1.992s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 6.601133, mae: 39.667374, mean_q: -58.773346\n",
      "  66000/150000: episode: 330, duration: 1.998s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 9.091408, mae: 39.617428, mean_q: -58.553787\n",
      "  66200/150000: episode: 331, duration: 1.959s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 7.313219, mae: 39.416882, mean_q: -58.430885\n",
      "  66400/150000: episode: 332, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 8.824607, mae: 39.186817, mean_q: -57.979549\n",
      "  66600/150000: episode: 333, duration: 1.944s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 8.690478, mae: 39.020958, mean_q: -57.851688\n",
      "  66800/150000: episode: 334, duration: 2.020s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.968360, mae: 38.982368, mean_q: -57.836212\n",
      "  67000/150000: episode: 335, duration: 2.173s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 9.080300, mae: 38.914307, mean_q: -57.666779\n",
      "  67200/150000: episode: 336, duration: 2.211s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 9.127995, mae: 38.689220, mean_q: -57.172539\n",
      "  67400/150000: episode: 337, duration: 2.299s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 6.959710, mae: 38.698322, mean_q: -57.399605\n",
      "  67600/150000: episode: 338, duration: 2.258s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 7.438772, mae: 38.787216, mean_q: -57.511269\n",
      "  67800/150000: episode: 339, duration: 2.549s, episode steps: 200, steps per second:  78, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 7.671900, mae: 38.727348, mean_q: -57.425217\n",
      "  68000/150000: episode: 340, duration: 2.308s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 9.613554, mae: 38.713013, mean_q: -57.239452\n",
      "  68200/150000: episode: 341, duration: 2.275s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 9.796361, mae: 38.574829, mean_q: -57.002697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  68400/150000: episode: 342, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 5.345106, mae: 38.373234, mean_q: -56.948574\n",
      "  68600/150000: episode: 343, duration: 2.142s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 9.482738, mae: 38.301041, mean_q: -56.671459\n",
      "  68800/150000: episode: 344, duration: 2.532s, episode steps: 200, steps per second:  79, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 7.829176, mae: 38.334564, mean_q: -56.855400\n",
      "  69000/150000: episode: 345, duration: 2.642s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 7.838324, mae: 38.299213, mean_q: -56.705887\n",
      "  69200/150000: episode: 346, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 7.176673, mae: 38.190571, mean_q: -56.629219\n",
      "  69400/150000: episode: 347, duration: 2.118s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 8.292996, mae: 38.205532, mean_q: -56.607666\n",
      "  69600/150000: episode: 348, duration: 2.029s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 10.161781, mae: 38.142769, mean_q: -56.353760\n",
      "  69800/150000: episode: 349, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.981173, mae: 38.069370, mean_q: -56.403214\n",
      "  70000/150000: episode: 350, duration: 2.077s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 6.087200, mae: 38.056366, mean_q: -56.443604\n",
      "  70200/150000: episode: 351, duration: 2.092s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 6.174862, mae: 38.152939, mean_q: -56.650352\n",
      "  70400/150000: episode: 352, duration: 2.098s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 6.824365, mae: 38.211472, mean_q: -56.624588\n",
      "  70600/150000: episode: 353, duration: 2.084s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 9.528362, mae: 38.114689, mean_q: -56.406090\n",
      "  70800/150000: episode: 354, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 8.502300, mae: 38.106075, mean_q: -56.364864\n",
      "  71000/150000: episode: 355, duration: 2.106s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 7.744544, mae: 37.945480, mean_q: -56.045685\n",
      "  71200/150000: episode: 356, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.062780, mae: 37.896797, mean_q: -56.160400\n",
      "  71400/150000: episode: 357, duration: 2.145s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 6.791307, mae: 37.834641, mean_q: -56.056610\n",
      "  71600/150000: episode: 358, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 6.961516, mae: 37.986702, mean_q: -56.347363\n",
      "  71800/150000: episode: 359, duration: 2.125s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 5.014227, mae: 38.120644, mean_q: -56.595810\n",
      "  72000/150000: episode: 360, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 4.286253, mae: 38.339039, mean_q: -56.996246\n",
      "  72200/150000: episode: 361, duration: 2.076s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.015809, mae: 38.464424, mean_q: -56.963547\n",
      "  72400/150000: episode: 362, duration: 2.053s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 7.967247, mae: 38.510540, mean_q: -56.941669\n",
      "  72600/150000: episode: 363, duration: 2.182s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 5.502978, mae: 38.611263, mean_q: -57.293880\n",
      "  72800/150000: episode: 364, duration: 2.074s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.106371, mae: 38.692963, mean_q: -57.311890\n",
      "  73000/150000: episode: 365, duration: 2.205s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 8.423115, mae: 38.660080, mean_q: -57.243900\n",
      "  73200/150000: episode: 366, duration: 2.213s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 6.797723, mae: 38.731491, mean_q: -57.446400\n",
      "  73400/150000: episode: 367, duration: 2.215s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 7.713412, mae: 38.861275, mean_q: -57.542870\n",
      "  73600/150000: episode: 368, duration: 2.192s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 9.500500, mae: 38.825336, mean_q: -57.381180\n",
      "  73800/150000: episode: 369, duration: 2.139s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 9.234489, mae: 38.732540, mean_q: -57.348652\n",
      "  74000/150000: episode: 370, duration: 2.218s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 5.177586, mae: 38.828651, mean_q: -57.647675\n",
      "  74200/150000: episode: 371, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.115782, mae: 38.864536, mean_q: -57.592361\n",
      "  74400/150000: episode: 372, duration: 2.124s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 6.677783, mae: 38.941296, mean_q: -57.675381\n",
      "  74600/150000: episode: 373, duration: 2.072s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 6.346101, mae: 39.021656, mean_q: -57.860489\n",
      "  74800/150000: episode: 374, duration: 2.117s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 6.095825, mae: 39.068539, mean_q: -57.933578\n",
      "  75000/150000: episode: 375, duration: 2.082s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.390674, mae: 39.000992, mean_q: -57.693993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  75200/150000: episode: 376, duration: 2.043s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 9.519953, mae: 38.989506, mean_q: -57.645351\n",
      "  75400/150000: episode: 377, duration: 2.189s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 8.571259, mae: 38.945423, mean_q: -57.572906\n",
      "  75600/150000: episode: 378, duration: 2.132s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.646763, mae: 38.790565, mean_q: -57.542206\n",
      "  75800/150000: episode: 379, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.209053, mae: 38.788769, mean_q: -57.412811\n",
      "  76000/150000: episode: 380, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 7.040565, mae: 38.643219, mean_q: -57.234356\n",
      "  76200/150000: episode: 381, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 5.434032, mae: 38.676472, mean_q: -57.357597\n",
      "  76400/150000: episode: 382, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.048882, mae: 38.637745, mean_q: -57.120785\n",
      "  76600/150000: episode: 383, duration: 2.183s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 7.876838, mae: 38.435017, mean_q: -56.813797\n",
      "  76800/150000: episode: 384, duration: 2.249s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.165308, mae: 38.253372, mean_q: -56.478992\n",
      "  77000/150000: episode: 385, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 5.912628, mae: 37.981522, mean_q: -56.219452\n",
      "  77200/150000: episode: 386, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 7.848212, mae: 37.744583, mean_q: -55.838154\n",
      "  77400/150000: episode: 387, duration: 2.282s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 8.853006, mae: 37.602001, mean_q: -55.565155\n",
      "  77600/150000: episode: 388, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 4.557631, mae: 37.508266, mean_q: -55.490692\n",
      "  77800/150000: episode: 389, duration: 2.308s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 6.078443, mae: 37.206730, mean_q: -55.075294\n",
      "  78000/150000: episode: 390, duration: 2.627s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 10.181731, mae: 37.152386, mean_q: -54.856396\n",
      "  78200/150000: episode: 391, duration: 2.290s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 7.837965, mae: 36.731022, mean_q: -54.123989\n",
      "  78400/150000: episode: 392, duration: 2.949s, episode steps: 200, steps per second:  68, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 5.092062, mae: 36.439068, mean_q: -54.007793\n",
      "  78600/150000: episode: 393, duration: 2.486s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 8.081955, mae: 36.421974, mean_q: -53.846561\n",
      "  78800/150000: episode: 394, duration: 2.637s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 4.605370, mae: 36.254139, mean_q: -53.678314\n",
      "  79000/150000: episode: 395, duration: 2.353s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 7.829991, mae: 36.018425, mean_q: -53.093399\n",
      "  79200/150000: episode: 396, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 5.686441, mae: 35.722610, mean_q: -52.790077\n",
      "  79400/150000: episode: 397, duration: 2.230s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000],  loss: 8.294597, mae: 35.445969, mean_q: -52.243008\n",
      "  79600/150000: episode: 398, duration: 2.324s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 7.094759, mae: 35.230080, mean_q: -52.047020\n",
      "  79800/150000: episode: 399, duration: 2.439s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 6.495204, mae: 35.100906, mean_q: -51.885895\n",
      "  80000/150000: episode: 400, duration: 2.371s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 6.417678, mae: 34.989799, mean_q: -51.713036\n",
      "  80200/150000: episode: 401, duration: 2.324s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 4.241981, mae: 34.985611, mean_q: -51.800793\n",
      "  80400/150000: episode: 402, duration: 2.321s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 4.950053, mae: 34.993568, mean_q: -51.765118\n",
      "  80600/150000: episode: 403, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 6.851456, mae: 34.939766, mean_q: -51.622223\n",
      "  80800/150000: episode: 404, duration: 2.276s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 7.401216, mae: 34.712189, mean_q: -51.254658\n",
      "  81000/150000: episode: 405, duration: 2.265s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 6.506302, mae: 34.641567, mean_q: -51.199718\n",
      "  81200/150000: episode: 406, duration: 2.312s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 3.919395, mae: 34.667671, mean_q: -51.406425\n",
      "  81400/150000: episode: 407, duration: 2.240s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 5.581111, mae: 34.715824, mean_q: -51.333584\n",
      "  81600/150000: episode: 408, duration: 2.253s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 5.578053, mae: 34.737804, mean_q: -51.414867\n",
      "  81800/150000: episode: 409, duration: 2.173s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 4.183102, mae: 34.947166, mean_q: -51.853233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  82000/150000: episode: 410, duration: 2.081s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 6.327131, mae: 35.001495, mean_q: -51.826359\n",
      "  82200/150000: episode: 411, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 5.656868, mae: 35.130318, mean_q: -52.060196\n",
      "  82400/150000: episode: 412, duration: 2.042s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 5.990932, mae: 35.476608, mean_q: -52.626591\n",
      "  82600/150000: episode: 413, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.715 [0.000, 2.000],  loss: 6.908877, mae: 35.611637, mean_q: -52.602974\n",
      "  82800/150000: episode: 414, duration: 2.139s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.765 [0.000, 2.000],  loss: 6.448442, mae: 35.654800, mean_q: -52.771816\n",
      "  83000/150000: episode: 415, duration: 2.218s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 5.748472, mae: 35.670361, mean_q: -52.769230\n",
      "  83200/150000: episode: 416, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 7.222394, mae: 35.530575, mean_q: -52.511719\n",
      "  83400/150000: episode: 417, duration: 2.092s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 6.623487, mae: 35.439503, mean_q: -52.408936\n",
      "  83600/150000: episode: 418, duration: 2.064s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.715 [0.000, 2.000],  loss: 5.920029, mae: 35.331959, mean_q: -52.181583\n",
      "  83800/150000: episode: 419, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.665 [0.000, 2.000],  loss: 4.952151, mae: 35.024033, mean_q: -51.701347\n",
      "  84000/150000: episode: 420, duration: 1.956s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.725 [0.000, 2.000],  loss: 5.256956, mae: 34.855621, mean_q: -51.494820\n",
      "  84200/150000: episode: 421, duration: 2.016s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.680 [0.000, 2.000],  loss: 6.054477, mae: 34.427933, mean_q: -50.779221\n",
      "  84400/150000: episode: 422, duration: 2.242s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.755 [0.000, 2.000],  loss: 6.228079, mae: 34.319038, mean_q: -50.649158\n",
      "  84600/150000: episode: 423, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 5.529264, mae: 34.039474, mean_q: -50.193134\n",
      "  84800/150000: episode: 424, duration: 2.314s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000],  loss: 6.201547, mae: 33.877182, mean_q: -49.921635\n",
      "  85000/150000: episode: 425, duration: 2.247s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.730 [0.000, 2.000],  loss: 7.426912, mae: 33.496685, mean_q: -49.340626\n",
      "  85200/150000: episode: 426, duration: 2.117s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.775 [0.000, 2.000],  loss: 5.464396, mae: 33.367638, mean_q: -49.275181\n",
      "  85400/150000: episode: 427, duration: 2.258s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 5.797055, mae: 33.101681, mean_q: -48.796066\n",
      "  85600/150000: episode: 428, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 4.709885, mae: 32.995567, mean_q: -48.706871\n",
      "  85800/150000: episode: 429, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 4.421246, mae: 33.045044, mean_q: -48.884697\n",
      "  86000/150000: episode: 430, duration: 2.130s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 4.465809, mae: 33.088726, mean_q: -48.940777\n",
      "  86200/150000: episode: 431, duration: 2.137s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 5.587031, mae: 33.266220, mean_q: -49.147438\n",
      "  86400/150000: episode: 432, duration: 2.190s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 6.502228, mae: 33.091396, mean_q: -48.838661\n",
      "  86600/150000: episode: 433, duration: 2.102s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 5.810802, mae: 33.170605, mean_q: -49.072754\n",
      "  86800/150000: episode: 434, duration: 2.254s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 4.143515, mae: 33.299313, mean_q: -49.306358\n",
      "  87000/150000: episode: 435, duration: 2.351s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 4.535582, mae: 33.564518, mean_q: -49.682911\n",
      "  87200/150000: episode: 436, duration: 2.307s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 4.103343, mae: 33.743797, mean_q: -49.994972\n",
      "  87400/150000: episode: 437, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 7.038967, mae: 33.649647, mean_q: -49.656517\n",
      "  87600/150000: episode: 438, duration: 2.079s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 5.319093, mae: 33.522720, mean_q: -49.477772\n",
      "  87800/150000: episode: 439, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 4.401624, mae: 33.569706, mean_q: -49.521606\n",
      "  88000/150000: episode: 440, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 4.060614, mae: 33.615406, mean_q: -49.638584\n",
      "  88200/150000: episode: 441, duration: 2.183s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 5.596274, mae: 33.341015, mean_q: -49.154343\n",
      "  88400/150000: episode: 442, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 4.866454, mae: 33.328541, mean_q: -49.137680\n",
      "  88600/150000: episode: 443, duration: 2.152s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 5.315587, mae: 33.157780, mean_q: -48.864902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  88800/150000: episode: 444, duration: 2.162s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 5.448565, mae: 32.920799, mean_q: -48.470226\n",
      "  89000/150000: episode: 445, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 3.746906, mae: 32.873989, mean_q: -48.423981\n",
      "  89200/150000: episode: 446, duration: 2.247s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 3.676062, mae: 32.701759, mean_q: -48.169388\n",
      "  89400/150000: episode: 447, duration: 2.229s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 3.676231, mae: 32.477772, mean_q: -47.882835\n",
      "  89600/150000: episode: 448, duration: 2.099s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 4.520416, mae: 32.371483, mean_q: -47.537689\n",
      "  89800/150000: episode: 449, duration: 2.245s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 5.475331, mae: 32.096710, mean_q: -47.198673\n",
      "  90000/150000: episode: 450, duration: 2.225s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 5.374184, mae: 31.768770, mean_q: -46.583168\n",
      "  90200/150000: episode: 451, duration: 2.120s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 5.559926, mae: 31.353426, mean_q: -46.097572\n",
      "  90400/150000: episode: 452, duration: 2.066s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 4.222185, mae: 31.177885, mean_q: -45.828121\n",
      "  90600/150000: episode: 453, duration: 2.221s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 4.407376, mae: 31.101423, mean_q: -45.843266\n",
      "  90800/150000: episode: 454, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 5.553266, mae: 31.132343, mean_q: -45.819576\n",
      "  91000/150000: episode: 455, duration: 2.157s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 3.268685, mae: 31.218801, mean_q: -46.118633\n",
      "  91200/150000: episode: 456, duration: 2.098s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 4.769782, mae: 31.330450, mean_q: -46.198574\n",
      "  91400/150000: episode: 457, duration: 2.121s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 5.275867, mae: 31.206682, mean_q: -46.017494\n",
      "  91600/150000: episode: 458, duration: 2.067s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 4.603450, mae: 31.304804, mean_q: -46.172482\n",
      "  91800/150000: episode: 459, duration: 2.142s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 4.596654, mae: 31.421795, mean_q: -46.315952\n",
      "  92000/150000: episode: 460, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.668491, mae: 31.301054, mean_q: -46.252388\n",
      "  92200/150000: episode: 461, duration: 2.203s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 4.193822, mae: 31.436207, mean_q: -46.328857\n",
      "  92400/150000: episode: 462, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.765 [0.000, 2.000],  loss: 5.268913, mae: 31.321796, mean_q: -46.068836\n",
      "  92600/150000: episode: 463, duration: 2.002s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000],  loss: 3.394615, mae: 31.309029, mean_q: -46.128868\n",
      "  92800/150000: episode: 464, duration: 2.120s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000],  loss: 4.076532, mae: 31.129404, mean_q: -45.794601\n",
      "  93000/150000: episode: 465, duration: 2.056s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 3.619969, mae: 30.901548, mean_q: -45.544411\n",
      "  93200/150000: episode: 466, duration: 1.977s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 5.730279, mae: 30.710596, mean_q: -45.129711\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dqn.fit(env, nb_steps=150000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('model.mdl', overwrite=True)\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneticAlgorithm",
   "language": "python",
   "name": "geneticalgorithm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
