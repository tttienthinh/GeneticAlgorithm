{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://bi3mer.github.io/blog/post_18/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 10,819\n",
      "Trainable params: 10,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(env.action_space.n))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tttienthinh/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.agents import DQNAgent\n",
    "\n",
    "dqn = DQNAgent(\n",
    "    model=model, \n",
    "    nb_actions=env.action_space.n, \n",
    "    memory=SequentialMemory(limit=50000, window_length=1), \n",
    "    nb_steps_warmup=10,\n",
    "    target_model_update=1e-2, \n",
    "    policy=BoltzmannQPolicy()\n",
    ")\n",
    "\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 150000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tttienthinh/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "/home/tttienthinh/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    200/150000: episode: 1, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.069910, mae: 0.804387, mean_q: -0.994688\n",
      "    400/150000: episode: 2, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.012567, mae: 1.743582, mean_q: -2.554291\n",
      "    600/150000: episode: 3, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.025732, mae: 2.846496, mean_q: -4.193378\n",
      "    800/150000: episode: 4, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.057200, mae: 3.982826, mean_q: -5.873297\n",
      "   1000/150000: episode: 5, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.091419, mae: 5.125338, mean_q: -7.548160\n",
      "   1200/150000: episode: 6, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.176833, mae: 6.228465, mean_q: -9.206479\n",
      "   1400/150000: episode: 7, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.236266, mae: 7.306727, mean_q: -10.755772\n",
      "   1600/150000: episode: 8, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.382296, mae: 8.334019, mean_q: -12.274677\n",
      "   1800/150000: episode: 9, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.399624, mae: 9.302969, mean_q: -13.736231\n",
      "   2000/150000: episode: 10, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.490755, mae: 10.297118, mean_q: -15.184639\n",
      "   2200/150000: episode: 11, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.477208, mae: 11.220292, mean_q: -16.609098\n",
      "   2400/150000: episode: 12, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.715126, mae: 12.142217, mean_q: -17.938770\n",
      "   2600/150000: episode: 13, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.849527, mae: 13.042244, mean_q: -19.258512\n",
      "   2800/150000: episode: 14, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 1.197667, mae: 13.831409, mean_q: -20.370968\n",
      "   3000/150000: episode: 15, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.375010, mae: 14.648493, mean_q: -21.570297\n",
      "   3200/150000: episode: 16, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.419737, mae: 15.432792, mean_q: -22.728374\n",
      "   3400/150000: episode: 17, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.012083, mae: 16.182789, mean_q: -23.962671\n",
      "   3600/150000: episode: 18, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 2.418431, mae: 16.918171, mean_q: -24.918615\n",
      "   3800/150000: episode: 19, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 1.479540, mae: 17.626535, mean_q: -26.118450\n",
      "   4000/150000: episode: 20, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 1.779327, mae: 18.355097, mean_q: -27.130997\n",
      "   4200/150000: episode: 21, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.786120, mae: 19.015854, mean_q: -28.113808\n",
      "   4400/150000: episode: 22, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 1.595482, mae: 19.635626, mean_q: -29.057178\n",
      "   4600/150000: episode: 23, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 2.361309, mae: 20.268997, mean_q: -29.928747\n",
      "   4800/150000: episode: 24, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 2.569810, mae: 20.813318, mean_q: -30.673708\n",
      "   5000/150000: episode: 25, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 2.382077, mae: 21.368198, mean_q: -31.615660\n",
      "   5200/150000: episode: 26, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 2.500371, mae: 21.965637, mean_q: -32.517941\n",
      "   5400/150000: episode: 27, duration: 1.329s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.712933, mae: 22.552158, mean_q: -33.340710\n",
      "   5600/150000: episode: 28, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 2.810313, mae: 23.050600, mean_q: -34.027454\n",
      "   5800/150000: episode: 29, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 3.257680, mae: 23.573753, mean_q: -34.848064\n",
      "   6000/150000: episode: 30, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 3.724244, mae: 23.992823, mean_q: -35.429943\n",
      "   6200/150000: episode: 31, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.957587, mae: 24.443104, mean_q: -36.232998\n",
      "   6400/150000: episode: 32, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 2.697155, mae: 24.959145, mean_q: -36.973145\n",
      "   6600/150000: episode: 33, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 3.953396, mae: 25.393557, mean_q: -37.533005\n",
      "   6800/150000: episode: 34, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 3.832479, mae: 25.847876, mean_q: -38.138046\n",
      "   7000/150000: episode: 35, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 3.567565, mae: 26.215721, mean_q: -38.777660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7200/150000: episode: 36, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 4.250932, mae: 26.688129, mean_q: -39.531445\n",
      "   7400/150000: episode: 37, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.937739, mae: 27.099297, mean_q: -40.117085\n",
      "   7600/150000: episode: 38, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 3.950129, mae: 27.450401, mean_q: -40.555809\n",
      "   7800/150000: episode: 39, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 3.305066, mae: 27.903048, mean_q: -41.356415\n",
      "   8000/150000: episode: 40, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 3.808476, mae: 28.322529, mean_q: -41.930050\n",
      "   8200/150000: episode: 41, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 4.601645, mae: 28.744545, mean_q: -42.580231\n",
      "   8400/150000: episode: 42, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 5.089758, mae: 29.083565, mean_q: -43.013329\n",
      "   8600/150000: episode: 43, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 4.543319, mae: 29.443769, mean_q: -43.566471\n",
      "   8800/150000: episode: 44, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 3.855151, mae: 29.728809, mean_q: -44.086197\n",
      "   9000/150000: episode: 45, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 4.174692, mae: 30.147034, mean_q: -44.689213\n",
      "   9200/150000: episode: 46, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 4.955883, mae: 30.473686, mean_q: -45.149487\n",
      "   9400/150000: episode: 47, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 5.087934, mae: 30.794355, mean_q: -45.604923\n",
      "   9600/150000: episode: 48, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 4.461004, mae: 31.057627, mean_q: -45.940693\n",
      "   9800/150000: episode: 49, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 5.427499, mae: 31.362238, mean_q: -46.318916\n",
      "  10000/150000: episode: 50, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 4.550709, mae: 31.548058, mean_q: -46.732277\n",
      "  10200/150000: episode: 51, duration: 1.575s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 2.880004, mae: 31.831270, mean_q: -47.364639\n",
      "  10400/150000: episode: 52, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 5.278852, mae: 32.184219, mean_q: -47.596996\n",
      "  10600/150000: episode: 53, duration: 1.712s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 4.566450, mae: 32.453773, mean_q: -48.061710\n",
      "  10800/150000: episode: 54, duration: 1.633s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 6.161200, mae: 32.674725, mean_q: -48.347450\n",
      "  11000/150000: episode: 55, duration: 1.653s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.655485, mae: 32.885548, mean_q: -48.626572\n",
      "  11200/150000: episode: 56, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 5.581527, mae: 33.043289, mean_q: -48.982113\n",
      "  11400/150000: episode: 57, duration: 1.722s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 5.681463, mae: 33.225609, mean_q: -49.257587\n",
      "  11600/150000: episode: 58, duration: 2.109s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 5.073795, mae: 33.481476, mean_q: -49.601830\n",
      "  11800/150000: episode: 59, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 5.931932, mae: 33.624062, mean_q: -49.779068\n",
      "  12000/150000: episode: 60, duration: 2.010s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 6.727425, mae: 33.724991, mean_q: -49.814960\n",
      "  12200/150000: episode: 61, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 6.538425, mae: 33.806122, mean_q: -49.888065\n",
      "  12400/150000: episode: 62, duration: 1.979s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 5.327307, mae: 33.872963, mean_q: -50.096485\n",
      "  12600/150000: episode: 63, duration: 1.922s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 6.325846, mae: 34.057693, mean_q: -50.400997\n",
      "  12800/150000: episode: 64, duration: 1.972s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 4.218305, mae: 34.191158, mean_q: -50.819794\n",
      "  13000/150000: episode: 65, duration: 2.029s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.133628, mae: 34.473732, mean_q: -50.988701\n",
      "  13200/150000: episode: 66, duration: 1.990s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 5.702402, mae: 34.699093, mean_q: -51.452492\n",
      "  13400/150000: episode: 67, duration: 1.925s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 6.643838, mae: 34.843746, mean_q: -51.606167\n",
      "  13600/150000: episode: 68, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 6.759688, mae: 34.894676, mean_q: -51.577255\n",
      "  13800/150000: episode: 69, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.347086, mae: 34.980640, mean_q: -51.810284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14000/150000: episode: 70, duration: 1.928s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 4.693427, mae: 35.217049, mean_q: -52.313652\n",
      "  14200/150000: episode: 71, duration: 1.945s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 7.421144, mae: 35.383717, mean_q: -52.398514\n",
      "  14400/150000: episode: 72, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 5.477354, mae: 35.538548, mean_q: -52.675816\n",
      "  14600/150000: episode: 73, duration: 1.945s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 7.124365, mae: 35.651695, mean_q: -52.805889\n",
      "  14800/150000: episode: 74, duration: 2.014s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.022964, mae: 35.871838, mean_q: -53.149590\n",
      "  15000/150000: episode: 75, duration: 1.978s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 5.320334, mae: 36.038998, mean_q: -53.388683\n",
      "  15200/150000: episode: 76, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 5.875841, mae: 36.276684, mean_q: -53.908279\n",
      "  15400/150000: episode: 77, duration: 1.955s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 6.318652, mae: 36.539459, mean_q: -54.110508\n",
      "  15600/150000: episode: 78, duration: 2.071s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 7.300470, mae: 36.715668, mean_q: -54.276474\n",
      "  15800/150000: episode: 79, duration: 1.935s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 8.691154, mae: 36.826370, mean_q: -54.413639\n",
      "  16000/150000: episode: 80, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 6.336411, mae: 36.937344, mean_q: -54.775829\n",
      "  16200/150000: episode: 81, duration: 2.123s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.062340, mae: 37.073524, mean_q: -54.847313\n",
      "  16400/150000: episode: 82, duration: 2.052s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 6.793336, mae: 37.083527, mean_q: -54.945488\n",
      "  16600/150000: episode: 83, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.108396, mae: 37.201538, mean_q: -55.141445\n",
      "  16800/150000: episode: 84, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 5.748633, mae: 37.350826, mean_q: -55.448963\n",
      "  17000/150000: episode: 85, duration: 1.981s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 9.323524, mae: 37.408459, mean_q: -55.270477\n",
      "  17200/150000: episode: 86, duration: 1.928s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.367958, mae: 37.439087, mean_q: -55.528759\n",
      "  17400/150000: episode: 87, duration: 2.001s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 7.406763, mae: 37.526619, mean_q: -55.628021\n",
      "  17600/150000: episode: 88, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.378608, mae: 37.684223, mean_q: -55.835609\n",
      "  17800/150000: episode: 89, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 6.396463, mae: 37.836868, mean_q: -56.154617\n",
      "  18000/150000: episode: 90, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 6.348165, mae: 38.010059, mean_q: -56.498104\n",
      "  18200/150000: episode: 91, duration: 1.884s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.488524, mae: 38.151047, mean_q: -56.613998\n",
      "  18400/150000: episode: 92, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 7.207521, mae: 38.314396, mean_q: -56.955196\n",
      "  18600/150000: episode: 93, duration: 2.107s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 8.095067, mae: 38.459606, mean_q: -56.958427\n",
      "  18800/150000: episode: 94, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 8.374751, mae: 38.555389, mean_q: -57.040844\n",
      "  19000/150000: episode: 95, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 6.875562, mae: 38.711197, mean_q: -57.499298\n",
      "  19200/150000: episode: 96, duration: 1.985s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 7.207747, mae: 38.790993, mean_q: -57.506107\n",
      "  19400/150000: episode: 97, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.474333, mae: 38.750011, mean_q: -57.422100\n",
      "  19600/150000: episode: 98, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 9.243856, mae: 38.765141, mean_q: -57.369507\n",
      "  19800/150000: episode: 99, duration: 2.077s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 6.362975, mae: 38.790279, mean_q: -57.635483\n",
      "  20000/150000: episode: 100, duration: 1.940s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 9.922456, mae: 38.912716, mean_q: -57.629318\n",
      "  20200/150000: episode: 101, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 6.716504, mae: 38.940540, mean_q: -57.850597\n",
      "  20400/150000: episode: 102, duration: 1.788s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 6.816788, mae: 39.078575, mean_q: -58.017696\n",
      "  20600/150000: episode: 103, duration: 1.929s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.889559, mae: 39.169640, mean_q: -57.980801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20800/150000: episode: 104, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 9.403881, mae: 39.088802, mean_q: -57.776768\n",
      "  21000/150000: episode: 105, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 7.847235, mae: 39.129921, mean_q: -57.987656\n",
      "  21200/150000: episode: 106, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 10.902219, mae: 39.077709, mean_q: -57.683800\n",
      "  21400/150000: episode: 107, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 7.019603, mae: 39.000702, mean_q: -57.860695\n",
      "  21600/150000: episode: 108, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 9.369748, mae: 39.215630, mean_q: -58.105125\n",
      "  21800/150000: episode: 109, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.498436, mae: 39.267921, mean_q: -58.178848\n",
      "  22000/150000: episode: 110, duration: 1.754s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 9.510682, mae: 39.302197, mean_q: -58.245392\n",
      "  22200/150000: episode: 111, duration: 1.768s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 5.800253, mae: 39.391232, mean_q: -58.507172\n",
      "  22400/150000: episode: 112, duration: 1.806s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 8.611917, mae: 39.508026, mean_q: -58.501537\n",
      "  22600/150000: episode: 113, duration: 1.765s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.020341, mae: 39.585861, mean_q: -58.735680\n",
      "  22800/150000: episode: 114, duration: 1.774s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 8.106196, mae: 39.684711, mean_q: -58.833813\n",
      "  23000/150000: episode: 115, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 12.198825, mae: 39.650665, mean_q: -58.580746\n",
      "  23200/150000: episode: 116, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 10.564457, mae: 39.633255, mean_q: -58.614079\n",
      "  23400/150000: episode: 117, duration: 2.060s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 10.088096, mae: 39.514881, mean_q: -58.451084\n",
      "  23600/150000: episode: 118, duration: 1.996s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 5.667644, mae: 39.670902, mean_q: -59.002735\n",
      "  23800/150000: episode: 119, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 9.837588, mae: 39.754845, mean_q: -58.773792\n",
      "  24000/150000: episode: 120, duration: 1.802s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 7.027730, mae: 39.832813, mean_q: -59.132511\n",
      "  24200/150000: episode: 121, duration: 1.805s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.060607, mae: 39.796230, mean_q: -58.871338\n",
      "  24400/150000: episode: 122, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 7.285475, mae: 39.822140, mean_q: -59.095245\n",
      "  24600/150000: episode: 123, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 8.614057, mae: 39.843548, mean_q: -59.060577\n",
      "  24800/150000: episode: 124, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 7.613134, mae: 39.956345, mean_q: -59.229580\n",
      "  25000/150000: episode: 125, duration: 1.800s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.323335, mae: 39.969662, mean_q: -59.105717\n",
      "  25200/150000: episode: 126, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.217037, mae: 39.936142, mean_q: -59.248009\n",
      "  25400/150000: episode: 127, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.392544, mae: 39.965714, mean_q: -59.247646\n",
      "  25600/150000: episode: 128, duration: 1.974s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 5.760690, mae: 40.095680, mean_q: -59.627529\n",
      "  25800/150000: episode: 129, duration: 2.014s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.855426, mae: 40.351810, mean_q: -59.710800\n",
      "  26000/150000: episode: 130, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.304710, mae: 40.307247, mean_q: -59.808266\n",
      "  26200/150000: episode: 131, duration: 1.943s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.875463, mae: 40.327126, mean_q: -59.620640\n",
      "  26400/150000: episode: 132, duration: 2.394s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.378906, mae: 40.400017, mean_q: -59.937325\n",
      "  26600/150000: episode: 133, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.947611, mae: 40.405361, mean_q: -59.912529\n",
      "  26800/150000: episode: 134, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.996579, mae: 40.472527, mean_q: -60.026249\n",
      "  27000/150000: episode: 135, duration: 2.086s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.492183, mae: 40.520550, mean_q: -59.995514\n",
      "  27200/150000: episode: 136, duration: 2.079s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 9.629940, mae: 40.381069, mean_q: -59.800873\n",
      "  27400/150000: episode: 137, duration: 2.414s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 9.425563, mae: 40.420494, mean_q: -59.908268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  27600/150000: episode: 138, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.565435, mae: 40.452793, mean_q: -60.042198\n",
      "  27800/150000: episode: 139, duration: 2.011s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 10.492897, mae: 40.455051, mean_q: -59.954678\n",
      "  28000/150000: episode: 140, duration: 1.950s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 6.526506, mae: 40.610634, mean_q: -60.322617\n",
      "  28200/150000: episode: 141, duration: 1.960s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 10.573242, mae: 40.655262, mean_q: -60.119755\n",
      "  28400/150000: episode: 142, duration: 2.023s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 7.263603, mae: 40.545456, mean_q: -60.269073\n",
      "  28600/150000: episode: 143, duration: 1.929s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 6.494232, mae: 40.634327, mean_q: -60.360645\n",
      "  28800/150000: episode: 144, duration: 2.003s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.136594, mae: 40.735806, mean_q: -60.406284\n",
      "  29000/150000: episode: 145, duration: 1.994s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 10.199188, mae: 40.745018, mean_q: -60.330864\n",
      "  29200/150000: episode: 146, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.193198, mae: 40.789093, mean_q: -60.480175\n",
      "  29400/150000: episode: 147, duration: 1.968s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 11.103637, mae: 40.813869, mean_q: -60.402832\n",
      "  29600/150000: episode: 148, duration: 2.020s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.973345, mae: 40.751808, mean_q: -60.376339\n",
      "  29800/150000: episode: 149, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 7.881754, mae: 40.919140, mean_q: -60.709816\n",
      "  30000/150000: episode: 150, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 9.069616, mae: 40.860569, mean_q: -60.544277\n",
      "  30200/150000: episode: 151, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 9.561937, mae: 40.924953, mean_q: -60.614426\n",
      "  30400/150000: episode: 152, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.015512, mae: 40.849403, mean_q: -60.635883\n",
      "  30600/150000: episode: 153, duration: 2.010s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 5.748627, mae: 40.952316, mean_q: -60.921017\n",
      "  30800/150000: episode: 154, duration: 2.009s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 8.646556, mae: 41.039448, mean_q: -60.891483\n",
      "  31000/150000: episode: 155, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 10.258300, mae: 41.074169, mean_q: -60.779575\n",
      "  31200/150000: episode: 156, duration: 2.421s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 13.363352, mae: 41.044815, mean_q: -60.580177\n",
      "  31400/150000: episode: 157, duration: 3.097s, episode steps: 200, steps per second:  65, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 5.460211, mae: 41.011009, mean_q: -60.966415\n",
      "  31600/150000: episode: 158, duration: 2.727s, episode steps: 200, steps per second:  73, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 10.444344, mae: 41.145943, mean_q: -60.968773\n",
      "  31800/150000: episode: 159, duration: 2.301s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 9.115100, mae: 41.111542, mean_q: -61.039806\n",
      "  32000/150000: episode: 160, duration: 2.387s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 7.221740, mae: 41.116581, mean_q: -61.123119\n",
      "  32200/150000: episode: 161, duration: 2.027s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 4.674744, mae: 41.281456, mean_q: -61.385738\n",
      "  32400/150000: episode: 162, duration: 2.018s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 9.609043, mae: 41.399445, mean_q: -61.391132\n",
      "  32600/150000: episode: 163, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 9.006224, mae: 41.475517, mean_q: -61.417919\n",
      "  32800/150000: episode: 164, duration: 2.010s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.201956, mae: 41.430164, mean_q: -61.276913\n",
      "  33000/150000: episode: 165, duration: 2.085s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 10.241900, mae: 41.415272, mean_q: -61.312401\n",
      "  33200/150000: episode: 166, duration: 2.488s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 6.508691, mae: 41.502018, mean_q: -61.738781\n",
      "  33400/150000: episode: 167, duration: 1.985s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.220400, mae: 41.584328, mean_q: -61.756073\n",
      "  33600/150000: episode: 168, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 9.459446, mae: 41.610615, mean_q: -61.664745\n",
      "  33800/150000: episode: 169, duration: 2.084s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 8.018139, mae: 41.609852, mean_q: -61.686554\n",
      "  34000/150000: episode: 170, duration: 1.951s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 10.132805, mae: 41.547062, mean_q: -61.506500\n",
      "  34200/150000: episode: 171, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 7.619855, mae: 41.573048, mean_q: -61.722889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  34400/150000: episode: 172, duration: 1.959s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.791001, mae: 41.648418, mean_q: -61.924911\n",
      "  34600/150000: episode: 173, duration: 1.962s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 6.834288, mae: 41.872997, mean_q: -62.198589\n",
      "  34800/150000: episode: 174, duration: 1.953s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 8.412276, mae: 42.006851, mean_q: -62.392967\n",
      "  35000/150000: episode: 175, duration: 2.026s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 13.170584, mae: 41.950665, mean_q: -61.975292\n",
      "  35200/150000: episode: 176, duration: 1.969s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 7.045178, mae: 41.787201, mean_q: -61.960167\n",
      "  35400/150000: episode: 177, duration: 2.067s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.566242, mae: 41.813778, mean_q: -61.888500\n",
      "  35600/150000: episode: 178, duration: 2.046s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.242807, mae: 41.851730, mean_q: -61.965366\n",
      "  35800/150000: episode: 179, duration: 2.005s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.623415, mae: 41.833778, mean_q: -62.022049\n",
      "  36000/150000: episode: 180, duration: 2.016s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 10.807138, mae: 41.899532, mean_q: -61.922588\n",
      "  36200/150000: episode: 181, duration: 1.957s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.273753, mae: 41.829784, mean_q: -61.928944\n",
      "  36400/150000: episode: 182, duration: 2.039s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.730330, mae: 41.843380, mean_q: -62.102890\n",
      "  36600/150000: episode: 183, duration: 2.027s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.352713, mae: 41.926571, mean_q: -62.150352\n",
      "  36800/150000: episode: 184, duration: 2.028s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 11.893874, mae: 41.916115, mean_q: -62.028648\n",
      "  37000/150000: episode: 185, duration: 2.069s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.552635, mae: 41.831070, mean_q: -61.974178\n",
      "  37200/150000: episode: 186, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.873062, mae: 41.971188, mean_q: -62.414913\n",
      "  37400/150000: episode: 187, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 7.423495, mae: 42.068653, mean_q: -62.455643\n",
      "  37600/150000: episode: 188, duration: 2.047s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 11.724140, mae: 41.983612, mean_q: -62.135998\n",
      "  37800/150000: episode: 189, duration: 2.017s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.471892, mae: 41.937775, mean_q: -62.147545\n",
      "  38000/150000: episode: 190, duration: 1.977s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 9.716196, mae: 41.879940, mean_q: -62.126064\n",
      "  38200/150000: episode: 191, duration: 2.061s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 9.702482, mae: 41.865704, mean_q: -61.984638\n",
      "  38400/150000: episode: 192, duration: 2.754s, episode steps: 200, steps per second:  73, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 9.577429, mae: 41.904472, mean_q: -62.077782\n",
      "  38600/150000: episode: 193, duration: 3.392s, episode steps: 200, steps per second:  59, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 10.630221, mae: 41.807560, mean_q: -61.946213\n",
      "  38800/150000: episode: 194, duration: 3.241s, episode steps: 200, steps per second:  62, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 6.921779, mae: 41.882549, mean_q: -62.249561\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dqn.fit(env, nb_steps=150000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('model.mdl', overwrite=True)\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneticAlgorithm",
   "language": "python",
   "name": "geneticalgorithm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
