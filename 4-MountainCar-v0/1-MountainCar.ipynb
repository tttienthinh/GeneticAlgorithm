{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_obs = env.observation_space.shape\n",
    "nb_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_act = env.action_space.n\n",
    "nb_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(1,) + nb_obs),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    \n",
    "    Dense(nb_act),\n",
    "    Activation('linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tttienthinh/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_act, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=BoltzmannQPolicy())\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tttienthinh/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "/home/tttienthinh/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  200/10000: episode: 1, duration: 2.218s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.058594, mae: 34.985944, mean_q: -52.278669\n",
      "  400/10000: episode: 2, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 3.985336, mae: 35.423386, mean_q: -52.336426\n",
      "  600/10000: episode: 3, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 5.391562, mae: 35.553185, mean_q: -52.532188\n",
      "  800/10000: episode: 4, duration: 1.701s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 5.632994, mae: 35.751575, mean_q: -52.863682\n",
      " 1000/10000: episode: 5, duration: 1.676s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 5.077288, mae: 35.899776, mean_q: -53.014034\n",
      " 1200/10000: episode: 6, duration: 1.740s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 7.735181, mae: 35.859879, mean_q: -52.757931\n",
      " 1400/10000: episode: 7, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 4.743564, mae: 35.808521, mean_q: -52.983536\n",
      " 1600/10000: episode: 8, duration: 1.680s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 6.808837, mae: 35.699120, mean_q: -52.618927\n",
      " 1800/10000: episode: 9, duration: 1.709s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 5.593582, mae: 35.501732, mean_q: -52.404892\n",
      " 2000/10000: episode: 10, duration: 1.734s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 10.411521, mae: 35.197384, mean_q: -51.702450\n",
      " 2200/10000: episode: 11, duration: 1.708s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.720 [0.000, 2.000],  loss: 8.520005, mae: 34.792015, mean_q: -51.188370\n",
      " 2400/10000: episode: 12, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000],  loss: 6.137578, mae: 34.425621, mean_q: -50.848961\n",
      " 2600/10000: episode: 13, duration: 1.741s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.785 [0.000, 2.000],  loss: 4.935349, mae: 34.541473, mean_q: -51.082817\n",
      " 2800/10000: episode: 14, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000],  loss: 8.297404, mae: 34.527035, mean_q: -50.900127\n",
      " 3000/10000: episode: 15, duration: 1.636s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.680 [0.000, 2.000],  loss: 6.387059, mae: 34.667603, mean_q: -51.030468\n",
      " 3200/10000: episode: 16, duration: 1.724s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 5.706201, mae: 34.615990, mean_q: -51.190849\n",
      " 3400/10000: episode: 17, duration: 1.724s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 6.867196, mae: 34.697399, mean_q: -51.153965\n",
      " 3600/10000: episode: 18, duration: 1.657s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 3.452840, mae: 34.924843, mean_q: -51.644588\n",
      " 3800/10000: episode: 19, duration: 1.697s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 8.724879, mae: 34.909283, mean_q: -51.191818\n",
      " 4000/10000: episode: 20, duration: 1.714s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 7.818656, mae: 34.954288, mean_q: -51.530941\n",
      " 4200/10000: episode: 21, duration: 1.695s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 6.504215, mae: 34.923565, mean_q: -51.446728\n",
      " 4400/10000: episode: 22, duration: 1.682s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 6.697901, mae: 34.910545, mean_q: -51.461586\n",
      " 4600/10000: episode: 23, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 6.710625, mae: 34.899559, mean_q: -51.440273\n",
      " 4800/10000: episode: 24, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 6.621140, mae: 34.958118, mean_q: -51.699318\n",
      " 5000/10000: episode: 25, duration: 1.670s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 7.861051, mae: 35.137054, mean_q: -51.720898\n",
      " 5200/10000: episode: 26, duration: 1.678s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 6.210316, mae: 35.194355, mean_q: -52.025753\n",
      " 5400/10000: episode: 27, duration: 1.719s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 3.908464, mae: 35.390202, mean_q: -52.452919\n",
      " 5600/10000: episode: 28, duration: 1.652s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.080100, mae: 35.711788, mean_q: -52.576893\n",
      " 5800/10000: episode: 29, duration: 1.667s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 6.831493, mae: 35.892799, mean_q: -52.974384\n",
      " 6000/10000: episode: 30, duration: 1.692s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 8.139255, mae: 35.994518, mean_q: -53.043461\n",
      " 6200/10000: episode: 31, duration: 1.686s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.115118, mae: 36.150284, mean_q: -53.370121\n",
      " 6400/10000: episode: 32, duration: 1.702s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 8.177018, mae: 36.236397, mean_q: -53.507793\n",
      " 6600/10000: episode: 33, duration: 1.678s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 4.832835, mae: 36.513901, mean_q: -54.187733\n",
      " 6800/10000: episode: 34, duration: 1.705s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.360665, mae: 36.785599, mean_q: -54.299240\n",
      " 7000/10000: episode: 35, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 6.595194, mae: 36.944660, mean_q: -54.733170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7200/10000: episode: 36, duration: 1.770s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 7.157710, mae: 37.121037, mean_q: -54.925102\n",
      " 7400/10000: episode: 37, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 6.109393, mae: 37.346012, mean_q: -55.328224\n",
      " 7600/10000: episode: 38, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.390052, mae: 37.558060, mean_q: -55.531738\n",
      " 7800/10000: episode: 39, duration: 1.696s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 8.045536, mae: 37.818653, mean_q: -55.899200\n",
      " 8000/10000: episode: 40, duration: 1.676s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 7.902440, mae: 37.893322, mean_q: -56.067307\n",
      " 8200/10000: episode: 41, duration: 1.975s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 9.208552, mae: 38.018112, mean_q: -56.126446\n",
      " 8400/10000: episode: 42, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 10.295933, mae: 38.048691, mean_q: -56.119240\n",
      " 8600/10000: episode: 43, duration: 1.732s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 6.896792, mae: 38.049156, mean_q: -56.290245\n",
      " 8800/10000: episode: 44, duration: 1.994s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 9.268098, mae: 38.204483, mean_q: -56.290565\n",
      " 9000/10000: episode: 45, duration: 1.759s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 6.116644, mae: 37.973347, mean_q: -56.197994\n",
      " 9200/10000: episode: 46, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 6.772870, mae: 38.009403, mean_q: -56.137218\n",
      " 9400/10000: episode: 47, duration: 1.717s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 5.220935, mae: 38.064468, mean_q: -56.445480\n",
      " 9600/10000: episode: 48, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 10.390761, mae: 38.063625, mean_q: -56.200706\n",
      " 9800/10000: episode: 49, duration: 1.775s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 5.895126, mae: 38.091122, mean_q: -56.335815\n",
      " 10000/10000: episode: 50, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 6.941491, mae: 38.116051, mean_q: -56.437496\n",
      "done, took 87.065 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb14eaa41f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=10_000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -200.000, steps: 200\n",
      "Episode 2: reward: -200.000, steps: 200\n",
      "Episode 3: reward: -200.000, steps: 200\n",
      "Episode 4: reward: -200.000, steps: 200\n",
      "Episode 5: reward: -200.000, steps: 200\n",
      "Episode 6: reward: -200.000, steps: 200\n",
      "Episode 7: reward: -200.000, steps: 200\n",
      "Episode 8: reward: -200.000, steps: 200\n",
      "Episode 9: reward: -200.000, steps: 200\n",
      "Episode 10: reward: -200.000, steps: 200\n",
      "Episode 11: reward: -200.000, steps: 200\n",
      "Episode 12: reward: -200.000, steps: 200\n",
      "Episode 13: reward: -200.000, steps: 200\n",
      "Episode 14: reward: -200.000, steps: 200\n",
      "Episode 15: reward: -200.000, steps: 200\n",
      "Episode 16: reward: -200.000, steps: 200\n",
      "Episode 17: reward: -200.000, steps: 200\n",
      "Episode 18: reward: -200.000, steps: 200\n",
      "Episode 19: reward: -200.000, steps: 200\n",
      "Episode 20: reward: -200.000, steps: 200\n",
      "Episode 21: reward: -200.000, steps: 200\n",
      "Episode 22: reward: -200.000, steps: 200\n",
      "Episode 23: reward: -200.000, steps: 200\n",
      "Episode 24: reward: -200.000, steps: 200\n",
      "Episode 25: reward: -200.000, steps: 200\n",
      "Episode 26: reward: -200.000, steps: 200\n",
      "Episode 27: reward: -200.000, steps: 200\n",
      "Episode 28: reward: -200.000, steps: 200\n",
      "Episode 29: reward: -200.000, steps: 200\n",
      "Episode 30: reward: -200.000, steps: 200\n",
      "Episode 31: reward: -200.000, steps: 200\n",
      "Episode 32: reward: -200.000, steps: 200\n",
      "Episode 33: reward: -200.000, steps: 200\n",
      "Episode 34: reward: -200.000, steps: 200\n",
      "Episode 35: reward: -200.000, steps: 200\n",
      "Episode 36: reward: -200.000, steps: 200\n",
      "Episode 37: reward: -200.000, steps: 200\n",
      "Episode 38: reward: -200.000, steps: 200\n",
      "Episode 39: reward: -200.000, steps: 200\n",
      "Episode 40: reward: -200.000, steps: 200\n",
      "Episode 41: reward: -200.000, steps: 200\n",
      "Episode 42: reward: -200.000, steps: 200\n",
      "Episode 43: reward: -200.000, steps: 200\n",
      "Episode 44: reward: -200.000, steps: 200\n",
      "Episode 45: reward: -200.000, steps: 200\n",
      "Episode 46: reward: -200.000, steps: 200\n",
      "Episode 47: reward: -200.000, steps: 200\n",
      "Episode 48: reward: -200.000, steps: 200\n",
      "Episode 49: reward: -200.000, steps: 200\n",
      "Episode 50: reward: -200.000, steps: 200\n",
      "Episode 51: reward: -200.000, steps: 200\n",
      "Episode 52: reward: -200.000, steps: 200\n",
      "Episode 53: reward: -200.000, steps: 200\n",
      "Episode 54: reward: -200.000, steps: 200\n",
      "Episode 55: reward: -200.000, steps: 200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1186665/2457950955.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"episode_reward\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Select an action.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[0;34m(self, state_batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4052\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4054\u001b[0;31m     fetched = self._callable_fn(*array_vals,\n\u001b[0m\u001b[1;32m   4055\u001b[0m                                 run_metadata=self.run_metadata)\n\u001b[1;32m   4056\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programmation/Scientist/GeneticAlgorithm/venv/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1481\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m                                                run_metadata_ptr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = dqn.test(env, nb_episodes=100, visualize=True)\n",
    "np.mean(results.history[\"episode_reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneticAlgorithm",
   "language": "python",
   "name": "geneticalgorithm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
